{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom itertools import groupby\nfrom tf.keras.layers import StringLookup\n\n# --- 1. Define Character Set and Mappings ---\n# This must be the same character set the model was trained on.\n# The extra element at the end is the CTC 'blank' token.\nCHARACTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\nBLANK_INDEX = len(CHARACTERS)\n\nchar_to_num = StringLookup(\n    vocabulary=CHARACTERS, mask_token=None\n)\nnum_to_char = StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\n# --- 2. Define the Model Architecture ---\n# This should be the exact same architecture that was used for training.\ndef build_model(input_shape, vocab_size):\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Simple CNN frontend\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    # Reshape for RNN\n    _, time_dim, freq_dim, channel_dim = x.shape\n    x = tf.keras.layers.Reshape((time_dim, freq_dim * channel_dim))(x)\n    \n    # RNN backend\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    \n    # Output layer\n    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# --- 3. CTC Decoding Function ---\ndef ctc_greedy_decode(predictions):\n    \"\"\"Decodes the output of a CTC model using a simple greedy algorithm.\"\"\"\n    # Use argmax to get the most likely character index at each time step\n    predicted_indices = np.argmax(predictions, axis=-1)\n    \n    # Collapse repeating characters and remove blank tokens\n    decoded_indices = []\n    for k, _ in groupby(predicted_indices):\n        if k != BLANK_INDEX: # Ignore the blank token\n            decoded_indices.append(k)\n            \n    # Convert the integer sequence back to a string\n    decoded_text_tensor = num_to_char(tf.constant(decoded_indices))\n    decoded_text = tf.strings.reduce_join(decoded_text_tensor).numpy().decode(\"utf-8\")\n    \n    return decoded_text\n\n# --- 4. Putting It All Together ---\n\n# a) Create a dummy log Mel spectrogram (replace this with your actual data)\n# Shape: (time_steps, num_mels, channels=1)\ndummy_spectrogram = np.random.rand(150, 80, 1).astype(np.float32)\n\n# b) Build the model and load weights\n# For this example, we use the randomly initialized weights.\n# In a real scenario, you would load your trained weights here.\n# model.load_weights('path/to/your/trained_weights.h5')\ninput_shape = dummy_spectrogram.shape\nmodel = build_model(input_shape, len(CHARACTERS))\n\nprint(\"--- Model Summary ---\")\nmodel.summary()\n\n# c) Perform Inference\n# The model expects a batch, so we add a batch dimension to our single spectrogram\ninput_spectrogram = np.expand_dims(dummy_spectrogram, axis=0)\npredictions = model.predict(input_spectrogram)\n\n# d) Decode the output to get the final text\ndecoded_text = ctc_greedy_decode(predictions[0]) # Get the first (and only) item from the batch\n\nprint(f\"\\nInput Spectrogram Shape: {dummy_spectrogram.shape}\")\nprint(f\"Model Output Shape (Probabilities): {predictions.shape}\")\nprint(f\"Decoded Text: '{decoded_text}'\")\nprint(\"\\nNote: The output is gibberish because the model has not been trained.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}