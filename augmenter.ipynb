{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed9c81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from audiomentations import AddGaussianNoise\n",
    "import random\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fd67b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#--------AUGMENTATION-----------#\n",
    "\n",
    "class Augmenter:\n",
    "\n",
    "    def __init__(self, sr=16000,\n",
    "                 noise_prob=0.5, noise_max_amp=0.01,\n",
    "                 reverb_prob=0.3, reverb_delay=0.025, reverb_decay=0.2,\n",
    "                 shuffle_prob=0.3, shuffle_segments=3,\n",
    "                 time_stretch_prob=0.3, time_stretch_range=(0.9, 1.1),\n",
    "                 gaps_prob=0.3, gaps_n=2, gaps_max_duration=0.3,\n",
    "                 freq_mask_prob=0.3, freq_mask_n=1,\n",
    "                 shuffle_seg_dur=0.08, shuffle_overlap=0.02, shuffle_local_range=3):\n",
    "\n",
    "        self.sr = sr\n",
    "        self.noise_aug = AddGaussianNoise(p=1.0, max_amplitude=noise_max_amp)\n",
    "\n",
    "        self.noise_prob = noise_prob\n",
    "        self.reverb_prob = reverb_prob\n",
    "        self.reverb_delay = reverb_delay\n",
    "        self.reverb_decay = reverb_decay\n",
    "        self.shuffle_prob = shuffle_prob\n",
    "        self.shuffle_segments = shuffle_segments\n",
    "        self.time_stretch_prob = time_stretch_prob\n",
    "        self.time_stretch_range = time_stretch_range\n",
    "        self.gaps_prob = gaps_prob\n",
    "        self.gaps_n = gaps_n\n",
    "        self.gaps_max_duration = gaps_max_duration\n",
    "        self.freq_mask_prob = freq_mask_prob\n",
    "        self.freq_mask_n = freq_mask_n\n",
    "        self.shuffle_seg_dur = shuffle_seg_dur\n",
    "        self.shuffle_overlap = shuffle_overlap\n",
    "        self.shuffle_local_range = shuffle_local_range\n",
    "\n",
    "    def augment(self, audio):\n",
    "        if not isinstance(audio, tf.Tensor):\n",
    "            audio = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "        #Set of distortions to be applied randomly with probabilities below\n",
    "        distortions = []\n",
    "\n",
    "        # 1. Noise\n",
    "        # Adds gaussian noise -> makes it slightly grainy\n",
    "        # Min max amplitude of noise - not set\n",
    "        # p=1.0 - always apply noise\n",
    "        if random.random() < self.noise_prob:\n",
    "            distortions.append('noise')\n",
    "\n",
    "        # 2. Reverb\n",
    "        # Echo effect\n",
    "        # Delay the audio by 0.1 -> reduce volume -> pad it to original length -> add\n",
    "        if random.random() < self.reverb_prob:\n",
    "            distortions.append('reverb')\n",
    "\n",
    "        # 3. Shuffle\n",
    "        # Break into n segments and concat them randomly\n",
    "        if random.random() < self.shuffle_prob:\n",
    "            distortions.append('shuffle')\n",
    "\n",
    "        # 4. Time stretch\n",
    "        # Randomly slows (0.9) or speeds (1.1) the audio / doesn't change pitch\n",
    "        if random.random() < self.time_stretch_prob:\n",
    "            distortions.append('time_stretch')\n",
    "\n",
    "        # 5. Missing Gaps\n",
    "        # Randomly insert silences/gaps in the audio\n",
    "        if random.random() < self.gaps_prob:\n",
    "            distortions.append('missing_gaps')\n",
    "\n",
    "        # 6. Frequency Masking\n",
    "        # Randomly masks a range of frequencies in the spectrogram\n",
    "        #Butterworth filter is better than applying freqeuncy masks on spectogram (which already has frequency bins) because real wrld freq loss occurs during sound capture/transmission, affecting the raw audio.\n",
    "        #Butterworth simulates this situation by removing frequency content from the waveform which can then go thru the rest of pipeline,\n",
    "        #additionally it affects the phase relations and harmonics naturally in contrast to the crude zeroing of freq bins in spectogram\n",
    "\n",
    "        if random.random() < self.freq_mask_prob:\n",
    "            distortions.append('frequency_masking')\n",
    "\n",
    "\n",
    "\n",
    "        # Apply selected distortions\n",
    "        for distortion in distortions:\n",
    "            if distortion == 'noise':\n",
    "                audio = self._add_noise(audio)\n",
    "\n",
    "            elif distortion == 'reverb':\n",
    "                audio = self._add_reverb(audio)\n",
    "\n",
    "            elif distortion == 'shuffle':\n",
    "                audio = self._segment_shuffle(audio)\n",
    "\n",
    "            elif distortion == 'time_stretch':\n",
    "                audio = self._time_stretch(audio)\n",
    "\n",
    "            elif distortion == 'missing_gaps':\n",
    "                audio = self._add_missing_gaps(audio)\n",
    "\n",
    "            elif distortion == 'frequency_masking':\n",
    "                audio = self._add_frequency_mask(audio)\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "    #Augmentation methods\n",
    "    def _add_noise(self, audio):\n",
    "        if isinstance(audio, tf.Tensor):\n",
    "          audio = audio.numpy()\n",
    "        return self.noise_aug(audio, self.sr)\n",
    "\n",
    "    def _add_reverb(self, audio):\n",
    "        delay = int(self.reverb_delay * self.sr)  # Delay in samples (0.05 sec)\n",
    "        reverb = tf.pad(audio * self.reverb_decay, [[delay, 0]]) #Amplitude scaling -> 0.2\n",
    "        reverb = reverb[:tf.shape(audio)[0]]\n",
    "        return audio + reverb\n",
    "\n",
    "    def _segment_shuffle(self, audio, n_segments=None):\n",
    "        # Previous method -> shuffle random large segments / unrealistic and destroys linguistic stuff\n",
    "        # if n_segments is None:\n",
    "        #     n_segments = self.shuffle_segments\n",
    "        # segments = np.array_split(audio, n_segments)\n",
    "        # np.random.shuffle(segments)\n",
    "        # return np.concatenate(segments)\n",
    "\n",
    "\n",
    "        #New method -> try to simulate temporal jitter / noise in the time domain -> split into micro-segments which are overlapping  -> the segments are shuffle locally within shuffle range\n",
    "        if isinstance(audio, tf.Tensor):\n",
    "          audio = audio.numpy()\n",
    "        seg_len = int(self.shuffle_seg_dur * self.sr)\n",
    "        overlap = int(self.shuffle_overlap * self.sr)\n",
    "        local_range = self.shuffle_local_range\n",
    "        segments = []\n",
    "        i = 0\n",
    "        while i < len(audio):\n",
    "            end = min(i + seg_len, len(audio))\n",
    "            segments.append(audio[i:end])\n",
    "            i += seg_len - overlap\n",
    "        n_regions = min(4, max(1, len(segments) // 10))\n",
    "        region_indices = random.sample(range(len(segments)), n_regions)\n",
    "        shuffled = segments.copy()\n",
    "        for r in region_indices:\n",
    "            for offset in range(-local_range, local_range + 1):\n",
    "                idx = r + offset\n",
    "                if 0 <= idx < len(segments):\n",
    "                    shift = random.randint(-local_range, local_range)\n",
    "                    new_idx = max(0, min(len(segments) - 1, idx + shift))\n",
    "                    shuffled[idx] = segments[new_idx]\n",
    "        return np.concatenate(shuffled)[:len(audio)]\n",
    "\n",
    "\n",
    "    def _time_stretch(self, audio):\n",
    "          if isinstance(audio, tf.Tensor):\n",
    "              audio = audio.numpy()\n",
    "          stretched = librosa.effects.time_stretch(audio, rate=random.uniform(*self.time_stretch_range))\n",
    "          return tf.constant(stretched, dtype=tf.float32)\n",
    "\n",
    "    def _add_missing_gaps(self, audio, n_gaps=None, max_gap_duration=None):\n",
    "        # if n_gaps is None:\n",
    "        #     n_gaps = self.gaps_n\n",
    "        # if max_gap_duration is None:\n",
    "        #     max_gap_duration = self.gaps_max_duration\n",
    "        # gap_audio = np.copy(audio)\n",
    "        # for _ in range(n_gaps):\n",
    "        #     gap_duration = random.uniform(0.1, max_gap_duration)\n",
    "        #     gap_samples = int(gap_duration * self.sr)\n",
    "        #     start = random.randint(0, max(1, len(audio) - gap_samples))\n",
    "        #     gap_audio[start:start + gap_samples] = 0\n",
    "        # return gap_audio\n",
    "\n",
    "        #New method -> fill gaps with low level noises and make edges smoother\n",
    "        if n_gaps is None:\n",
    "            n_gaps = self.gaps_n\n",
    "        if max_gap_duration is None:\n",
    "            max_gap_duration = self.gaps_max_duration\n",
    "\n",
    "        gap_audio = tf.identity(audio)\n",
    "\n",
    "        for _ in range(n_gaps):\n",
    "            gap_duration = tf.random.uniform([], 0.1, max_gap_duration, dtype=tf.float32)\n",
    "            gap_samples = tf.cast(gap_duration * tf.cast(self.sr, tf.float32), tf.int32)\n",
    "            start = tf.random.uniform([], 0, tf.shape(audio)[0] - gap_samples, dtype=tf.int32)\n",
    "\n",
    "            fade_len = tf.minimum(tf.cast(0.05 * tf.cast(gap_samples, tf.float32), tf.int32), gap_samples // 4)\n",
    "            fade_out = tf.cast(tf.linspace(1.0, 0.0, fade_len), tf.float32)\n",
    "            fade_in = tf.cast(tf.linspace(0.0, 1.0, fade_len), tf.float32)\n",
    "\n",
    "            mid_len = gap_samples - 2 * fade_len\n",
    "\n",
    "            if mid_len > 0:\n",
    "            #In case 0 values are needed.\n",
    "            #     if random.random() < 0.5:\n",
    "            #         gap_audio[start + fade_len:start + fade_len + mid_len] = 0\n",
    "            #     else:\n",
    "              noise = tf.random.normal([mid_len], stddev=0.001, dtype=tf.float32)\n",
    "              indices = tf.reshape(tf.range(start + fade_len, start + fade_len + mid_len), (-1, 1))\n",
    "              gap_audio = tf.tensor_scatter_nd_update(gap_audio, indices, noise)\n",
    "\n",
    "        fade_indices = tf.reshape(tf.range(start + fade_len + mid_len, start + gap_samples), (-1, 1))\n",
    "        fade_vals = gap_audio[start + fade_len + mid_len:start + gap_samples] * fade_in\n",
    "        gap_audio = tf.tensor_scatter_nd_update(gap_audio, fade_indices, fade_vals)\n",
    "\n",
    "        return gap_audio[:len(audio)]\n",
    "    #APPLY FREQUENCY MASKING USING BUTTERWORTH FILTER\n",
    "    #n_masks -> how many bands will be filtered out\n",
    "    #A 16 kHz sampler can only capture up to 8 kHz frequencies because you need at least two samples per wave cycle to know what the wave looks like -> nyquist\n",
    "    def _add_frequency_mask(self, audio, n_masks=None):\n",
    "      if n_masks is None:\n",
    "          n_masks = self.freq_mask_n\n",
    "      if isinstance(audio, tf.Tensor):\n",
    "        masked_audio = audio.numpy().copy()\n",
    "      else:\n",
    "        masked_audio = audio.copy()\n",
    "      nyquist = self.sr/2\n",
    "      for _ in range(n_masks):\n",
    "        l_freq = random.uniform(500,5000)\n",
    "        h_freq = l_freq + random.uniform(500,2000)\n",
    "        l_freq = min(l_freq,nyquist-100)\n",
    "        h_freq = min(h_freq,nyquist-100)\n",
    "        b,a = butter(N=4, Wn=[l_freq,h_freq], btype= \"bandstop\", fs = self.sr) #N=4 th order -> a dip in frequency response where that removal band is with smooth edges\n",
    "        masked_audio = lfilter(b,a,masked_audio)\n",
    "      return masked_audio\n",
    "\n",
    "augmenter = Augmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77669b1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def mel_log_gen(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    audio = augmenter.augment(audio)\n",
    "    if isinstance(audio, tf.Tensor):\n",
    "        audio = audio.numpy()\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=80)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    return log_mel_spectrogram"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
