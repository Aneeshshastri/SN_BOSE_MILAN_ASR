{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cb4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Scanning directories for .flac files...\n",
      "Created a dataset with 28539 samples.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AddGaussianNoise.__init__() got an unexpected keyword argument 'sample_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated a dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hf_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 2. Instantiate dependencies for Citrinet\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m augmenter \u001b[38;5;241m=\u001b[39m Augmenter()\n\u001b[0;32m     99\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/stt_en_citrinet_256_ls\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# <-- CHANGED to the Citrinet model\u001b[39;00m\n\u001b[0;32m    100\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m, in \u001b[0;36mAugmenter.__init__\u001b[1;34m(self, sr, noise_prob, noise_max_amp, reverb_prob, reverb_delay, reverb_decay, shuffle_prob, time_stretch_prob, time_stretch_range, gaps_prob, gaps_n, gaps_max_duration, freq_mask_prob, freq_mask_n)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[0;32m     23\u001b[0m              noise_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, noise_max_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     24\u001b[0m              reverb_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, reverb_delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.025\u001b[39m, reverb_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     25\u001b[0m              shuffle_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, time_stretch_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, time_stretch_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m1.1\u001b[39m),\n\u001b[0;32m     26\u001b[0m              gaps_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08\u001b[39m, gaps_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, gaps_max_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     27\u001b[0m              freq_mask_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, freq_mask_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverb_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_stretch_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaps_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_mask_prob \u001b[38;5;241m=\u001b[39m sr, noise_prob, reverb_prob, shuffle_prob, time_stretch_prob, gaps_prob, freq_mask_prob\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_aug \u001b[38;5;241m=\u001b[39m AddGaussianNoise(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_amplitude\u001b[38;5;241m=\u001b[39mnoise_max_amp, sample_rate\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverb_delay, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverb_decay, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_stretch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaps_n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaps_max_duration, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_mask_n \u001b[38;5;241m=\u001b[39m reverb_delay, reverb_decay, time_stretch_range, gaps_n, gaps_max_duration, freq_mask_n\n",
      "\u001b[1;31mTypeError\u001b[0m: AddGaussianNoise.__init__() got an unexpected keyword argument 'sample_rate'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Import all required modules\n",
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoProcessor, AutoModelForCTC, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "from scipy.signal import butter, lfilter\n",
    "from audiomentations import AddGaussianNoise\n",
    "import torch\n",
    "\n",
    "# This import is not strictly needed for the Trainer but good to have for TF config\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 3: Your Augmenter class (remains the same)\n",
    "Training_dirs = \"./dataset/LibriSpeech/train-clean-100/\"\n",
    "\n",
    "class Augmenter:\n",
    "    # ... (Your full Augmenter class code remains here as provided) ...\n",
    "    def __init__(self, sr=16000,\n",
    "                 noise_prob=0.4, noise_max_amp=0.01,\n",
    "                 reverb_prob=0.3, reverb_delay=0.025, reverb_decay=0.2,\n",
    "                 shuffle_prob=0.05, time_stretch_prob=0.2, time_stretch_range=(0.9, 1.1),\n",
    "                 gaps_prob=0.08, gaps_n=4, gaps_max_duration=0.1,\n",
    "                 freq_mask_prob=0.2, freq_mask_n=1):\n",
    "        self.sr, self.noise_prob, self.reverb_prob, self.shuffle_prob, self.time_stretch_prob, self.gaps_prob, self.freq_mask_prob = sr, noise_prob, reverb_prob, shuffle_prob, time_stretch_prob, gaps_prob, freq_mask_prob\n",
    "        self.noise_aug = AddGaussianNoise(p=1.0, max_amplitude=noise_max_amp, sample_rate=sr)\n",
    "        self.reverb_delay, self.reverb_decay, self.time_stretch_range, self.gaps_n, self.gaps_max_duration, self.freq_mask_n = reverb_delay, reverb_decay, time_stretch_range, gaps_n, gaps_max_duration, freq_mask_n\n",
    "    def augment(self, audio):\n",
    "        distortions, audio = [], np.array(audio, dtype=np.float32)\n",
    "        if random.random() < self.noise_prob: distortions.append('noise')\n",
    "        if random.random() < self.reverb_prob: distortions.append('reverb')\n",
    "        if random.random() < self.shuffle_prob: distortions.append('shuffle')\n",
    "        if random.random() < self.time_stretch_prob: distortions.append('time_stretch')\n",
    "        if random.random() < self.gaps_prob: distortions.append('missing_gaps')\n",
    "        if random.random() < self.freq_mask_prob: distortions.append('frequency_masking')\n",
    "        for d in distortions:\n",
    "            if d == 'noise': audio = self.noise_aug(samples=audio, sample_rate=self.sr)\n",
    "            elif d == 'reverb':\n",
    "                delay, reverb = int(self.reverb_delay*self.sr), np.pad(audio*self.reverb_decay, (int(self.reverb_delay*self.sr), 0), 'constant')\n",
    "                audio += reverb[:len(audio)]\n",
    "            elif d == 'shuffle':\n",
    "                segs = np.array_split(audio, 3)\n",
    "                random.shuffle(segs)\n",
    "                audio = np.concatenate(segs)\n",
    "            elif d == 'time_stretch': audio = librosa.effects.time_stretch(y=audio, rate=random.uniform(*self.time_stretch_range))\n",
    "            elif d == 'missing_gaps':\n",
    "                gap_audio = np.copy(audio)\n",
    "                for _ in range(self.gaps_n):\n",
    "                    gap_dur, gap_samples = random.uniform(0.1, self.gaps_max_duration), int(random.uniform(0.1, self.gaps_max_duration) * self.sr)\n",
    "                    if len(gap_audio) > gap_samples:\n",
    "                        start = random.randint(0, len(gap_audio) - gap_samples)\n",
    "                        gap_audio[start:start+gap_samples] = 0\n",
    "                audio = gap_audio\n",
    "            elif d == 'frequency_masking':\n",
    "                nyquist = self.sr / 2\n",
    "                for _ in range(self.freq_mask_n):\n",
    "                    l_freq, h_freq = random.uniform(500, 5000), random.uniform(500, 2000)\n",
    "                    if l_freq + h_freq < nyquist:\n",
    "                        b, a = butter(N=4, Wn=[l_freq, l_freq + h_freq], btype=\"bandstop\", fs=self.sr)\n",
    "                        audio = lfilter(b, a, audio)\n",
    "        return audio\n",
    "\n",
    "# Step 4: Your custom load_data function (remains the same)\n",
    "def load_data():\n",
    "    file_paths, transcriptions = [], []\n",
    "    print(\"Scanning directories for .flac files...\")\n",
    "    for speaker_id in os.listdir(Training_dirs):\n",
    "        speaker_path = os.path.join(Training_dirs, speaker_id)\n",
    "        if not os.path.isdir(speaker_path): continue\n",
    "        for chapter_id in os.listdir(speaker_path):\n",
    "            chapter_path = os.path.join(speaker_path, chapter_id)\n",
    "            if not os.path.isdir(chapter_path): continue\n",
    "            trans_file = f\"{speaker_id}-{chapter_id}.trans.txt\"\n",
    "            trans_path = os.path.join(chapter_path, trans_file)\n",
    "            if os.path.exists(trans_path):\n",
    "                with open(trans_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split(' ', 1)\n",
    "                        file_id, text = parts[0], parts[1]\n",
    "                        audio_path = os.path.join(chapter_path, f\"{file_id}.flac\")\n",
    "                        if os.path.exists(audio_path):\n",
    "                            file_paths.append(audio_path)\n",
    "                            transcriptions.append(text)\n",
    "    return file_paths, transcriptions\n",
    "\n",
    "# --- Main Fine-Tuning Workflow for Citrinet ---\n",
    "\n",
    "# 1. Load data and create Dataset object\n",
    "file_paths, transcriptions = load_data()\n",
    "data_dict = {\"file_path\": file_paths, \"transcription\": transcriptions}\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "print(f\"Created a dataset with {len(hf_dataset)} samples.\")\n",
    "\n",
    "# 2. Instantiate dependencies for Citrinet\n",
    "augmenter = Augmenter()\n",
    "model_id = \"nvidia/stt_en_citrinet_256_ls\" # <-- CHANGED to the Citrinet model\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForCTC.from_pretrained(model_id)\n",
    "\n",
    "# 3. Create the preprocessing function (remains the same)\n",
    "def prepare_dataset(batch):\n",
    "    audio, sr = librosa.load(batch[\"file_path\"], sr=16000)\n",
    "    augmented_audio = augmenter.augment(audio)\n",
    "    batch[\"input_values\"] = processor(audio=augmented_audio, sampling_rate=16000).input_values[0]\n",
    "    batch[\"labels\"] = processor(text=batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "processed_ds = hf_dataset.map(prepare_dataset, remove_columns=hf_dataset.column_names, num_proc=1)\n",
    "\n",
    "# 4. Define the Data Collator (remains the same)\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = True\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        batch = processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        labels_batch = processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# 5. Define Training Arguments, adjusted for Citrinet\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./citrinet-finetuned-augmented\", # <-- CHANGED output directory\n",
    "    per_device_train_batch_size=16, # <-- CHANGED batch size (Citrinet is smaller)\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=10, # <-- CHANGED epochs (convolutional models can fine-tune quickly)\n",
    "    fp16=True,\n",
    "    learning_rate=1e-4, # <-- CHANGED learning rate (often better for Citrinet/QuartzNet)\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 6. Instantiate the Trainer (remains the same)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# 7. Start Fine-Tuning\n",
    "print(\"\\n--- Starting Model Fine-Tuning with Citrinet and Trainer ---\")\n",
    "trainer.train()\n",
    "print(\"\\n--- Fine-Tuning Complete ---\")\n",
    "\n",
    "# 8. Save the final model\n",
    "trainer.save_model(\"./citrinet-final-model\") # <-- CHANGED save path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
