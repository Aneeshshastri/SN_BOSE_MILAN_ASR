{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13471303,"sourceType":"datasetVersion","datasetId":8551657}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install librosa","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport os\nimport shutil\nfrom pathlib import Path\nimport random\nfrom scipy.signal import butter, lfilter\nfrom IPython.display import Audio\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:30:07.662067Z","iopub.execute_input":"2025-10-24T05:30:07.662298Z","iopub.status.idle":"2025-10-24T05:30:13.055710Z","shell.execute_reply.started":"2025-10-24T05:30:07.662280Z","shell.execute_reply":"2025-10-24T05:30:13.054909Z"}},"outputs":[{"name":"stderr","text":"2025-10-24 05:30:08.652112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761283808.681543     549 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761283808.691374     549 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# --- 1. Configuration ---\ntf.config.optimizer.set_experimental_options({'layout_optimizer': False})\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ninput_root_dir=\"/kaggle/input/milan2025asr/dataset_ASR/data_aug\"\noutput_root_dir = \"/kaggle/working/wav_asr\"\n# ---\nTraining_dirs=output_root_dir\n# Parameters\nSAMPLE_RATE = 16000\nBATCH_SIZE = 32\nEPOCHS = 15 # Set to a higher number for real training\n\n# Create a dummy vocabulary with only uppercase letters and apostrophe\nCHARACTERS = [\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    \"'\",' ']\n\n# Create character-to-number mappings\nchar_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\nnum_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\nVOCAB_SIZE = char_to_num.vocabulary_size()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:30:20.694579Z","iopub.execute_input":"2025-10-24T05:30:20.695121Z","iopub.status.idle":"2025-10-24T05:30:21.190867Z","shell.execute_reply.started":"2025-10-24T05:30:20.695094Z","shell.execute_reply":"2025-10-24T05:30:21.189964Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1761283821.139277     549 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 2. Set the path where you want to save the new dataset (with WAV files)\n# code to convert the augmented flac dataset to the new \n\n\nprint(f\"Starting transfer and conversion from '{input_root_dir}'...\")\nprint(f\"Output will be saved in '{output_root_dir}'.\")\n\n# Walk through the entire directory structure\nfor dirpath, _, filenames in os.walk(input_root_dir):\n    for filename in filenames:\n        # Construct the full path to the source file\n        input_file_path = os.path.join(dirpath, filename)\n        \n        # Determine the corresponding output directory path\n        relative_path = os.path.relpath(dirpath, input_root_dir)\n        output_dir = os.path.join(output_root_dir, relative_path)\n        \n        # Create the output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        # --- Logic to either convert or copy the file ---\n        try:\n            if filename.lower().endswith(\".flac\"):\n                # It's an audio file, so convert it to WAV\n                \n                # Create the full path for the output WAV file\n                wav_filename = Path(filename).stem + \".wav\"\n                output_file_path = os.path.join(output_dir, wav_filename)\n\n                # Read the FLAC data and write it as WAV\n                # Using soundfile which handles both reading FLAC and writing WAV\n                audio_data, sample_rate = sf.read(input_file_path)\n                sf.write(output_file_path, audio_data, sample_rate)\n                # Optional: Print progress for audio files\n                # print(f\"Converted: {input_file_path} -> {output_file_path}\")\n\n            else:\n                # It's a non-audio file (e.g., .txt), so copy it directly\n                \n                # Construct the output path for the copied file\n                output_file_path = os.path.join(output_dir, filename)\n                \n                # Use shutil.copy2 to preserve file metadata (like modification time)\n                shutil.copy2(input_file_path, output_file_path)\n                # Optional: Print progress for copied files\n                # print(f\"Copied: {input_file_path} -> {output_file_path}\")\n\n        except Exception as e:\n            # Print an error message if any file fails to process\n            print(f\"Error processing {input_file_path}: {e}\")\n\nprint(\"\\n--- Process Complete ---\")\nprint(f\"New dataset with WAV files is ready at: '{output_root_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:30:26.669636Z","iopub.execute_input":"2025-10-24T05:30:26.670029Z","iopub.status.idle":"2025-10-24T05:30:30.779355Z","shell.execute_reply.started":"2025-10-24T05:30:26.669999Z","shell.execute_reply":"2025-10-24T05:30:30.778387Z"}},"outputs":[{"name":"stdout","text":"Starting transfer and conversion from '/kaggle/input/milan2025asr/dataset_ASR/data_aug'...\nOutput will be saved in '/kaggle/working/wav_asr'.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_549/636006676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Read the FLAC data and write it as WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# Using soundfile which handles both reading FLAC and writing WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0maudio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Optional: Print progress for audio files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    305\u001b[0m     with SoundFile(file, 'r', samplerate, channels,\n\u001b[1;32m    306\u001b[0m                    subtype, endian, format, closefd) as f:\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_prepare_read\u001b[0;34m(self, start, stop, frames)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEEK_SET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, frames, whence)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \"\"\"\n\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_seek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         \u001b[0m_error_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Function to create dummy data\ndef load_data():\n    file_paths = []\n    transcriptions = []\n    directories=[]\n    label_files=[]\n    for lfold1 in os.listdir(Training_dirs):\n       if True:\n        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n            if os.path.isdir(full_path):\n                directories.append(full_path)\n                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n    for label_path in label_files:\n        with open(label_path,'r') as labels:\n            for line in labels.readlines():\n                transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n    for path in directories:\n        fp=[]\n        for file in os.listdir(path):\n            if(file.endswith('.wav')):\n                fp.append(os.path.join(path,file))\n        fp.sort()\n        file_paths+=fp\n    \n    print(len(label_files))\n        \n    return file_paths, transcriptions\n#print(os.listdir(Training_dirs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:30:53.089476Z","iopub.execute_input":"2025-10-24T05:30:53.090208Z","iopub.status.idle":"2025-10-24T05:30:53.096321Z","shell.execute_reply.started":"2025-10-24T05:30:53.090180Z","shell.execute_reply":"2025-10-24T05:30:53.095429Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- 2. tf.data Pipeline (No Augmentation) ---\n'''\ndef preprocess_audio(file_path):\n    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n    try:\n        path_str = file_path.numpy().decode('utf-8')\n        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n        \n        # Compute the Mel spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n        \n        # Convert to log scale (decibels)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Add a channel dimension\n        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n        \n        return log_mel_spec.astype(np.float32)\n    except Exception as e:\n        print(f\"Error processing file {file_path.numpy()}: {e}\")\n        os.exit()\n        return np.zeros((100, 80, 1), dtype=np.float32)\n'''       \n\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nN_MELS = 80\n\ndef power_to_db(S, ref=1.0, top_db=80.0):\n    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n\n@tf.function\ndef preprocess_audio_tf(file_path: tf.Tensor):\n    \"\"\"\n    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n    with padding to match librosa's default behavior.\n    \"\"\"\n    try:\n        \n        audio_binary = tf.io.read_file(file_path)\n    \n        # decode_wav returns a normalized float32 tensor and the sample rate.\n        # desired_channels=1 ensures the audio is mono.\n        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n    \n        # Squeeze the channel dimension, leaving a 1D waveform.\n        # NO further normalization is needed.\n        waveform = tf.squeeze(audio_tensor, axis=-1)\n\n        # --- FIX: Manually pad the waveform to match librosa ---\n        # (The rest of your function remains the same and is correct)\n        padding = N_FFT // 2\n        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n        \n        # --- 2. Compute the STFT (The rest is the same) ---\n        stft = tf.signal.stft(\n            waveform,\n            frame_length=N_FFT,\n            frame_step=HOP_LENGTH,\n            fft_length=N_FFT\n        )\n        spectrogram = tf.abs(stft)\n\n        # ... (rest of the function is identical) ...\n        power_spectrogram = spectrogram ** 2\n        num_spectrogram_bins = stft.shape[-1]\n        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n            num_mel_bins=N_MELS,\n            num_spectrogram_bins=num_spectrogram_bins,\n            sample_rate=SAMPLE_RATE,\n            lower_edge_hertz=20.0,\n            upper_edge_hertz=8000.0\n        )\n        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n        log_mel_spectrogram = power_to_db(mel_spectrogram)\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n\n        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n\n    except Exception as e:\n        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n    \ndef preprocess_label(text_label):\n    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n    # Convert all characters to uppercase to match the vocabulary\n    text_tensor = tf.strings.upper(text_label)\n    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n    return char_to_num(chars)\n# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n\n@tf.function\ndef preprocess_and_filter(path, label):\n    \"\"\"\n    Applies full preprocessing to audio and text, and returns their lengths.\n    \"\"\"\n    # Process the audio file to get the final spectrogram\n    spectrogram = preprocess_audio_tf(path)\n    \n    # Process the text label to get the integer tokens\n    processed_label = preprocess_label(label)\n\n    # Get the number of time steps from the spectrogram\n    spectrogram_length = tf.shape(spectrogram)[0]\n    \n    # Get the number of characters/tokens from the label\n    label_length = tf.shape(processed_label)[0]\n\n    return spectrogram, processed_label, spectrogram_length, label_length\n#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:37:04.819891Z","iopub.execute_input":"2025-10-24T05:37:04.820449Z","iopub.status.idle":"2025-10-24T05:37:04.831212Z","shell.execute_reply.started":"2025-10-24T05:37:04.820424Z","shell.execute_reply":"2025-10-24T05:37:04.830468Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"'''def build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=len(paths))\n    \n    # Map preprocessing functions\n    ds = ds.map(\n        lambda path, label: (\n            tf.py_function(preprocess_audio, [path], tf.float32),\n            preprocess_label(label)\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # Batch and pad\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds'''\n\ndef build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=2048)\n\n    # 1. Map the combined preprocessing and length calculation function\n    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # 2. Filter out items where the spectrogram is shorter than the label\n    \"\"\" ds = ds.filter(\n        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n    )\n    \n    # 3. Filter out items where the spectrogram time dimension is too short\n    MIN_SPEC_LENGTH=8\n    #print(f\"Filtering out spectrograms shorter than {MIN_SPEC_LENGTH} time steps.\")\n   ds = ds.filter(\n        lambda spectrogram, label, spec_len, label_len: spec_len >= MIN_SPEC_LENGTH\n    )\"\"\"\n    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n    ds = ds.map(\n        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # 4. Batch and pad as before\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:37:12.206787Z","iopub.execute_input":"2025-10-24T05:37:12.207064Z","iopub.status.idle":"2025-10-24T05:37:12.213298Z","shell.execute_reply.started":"2025-10-24T05:37:12.207044Z","shell.execute_reply":"2025-10-24T05:37:12.212487Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def build_model(input_shape, vocab_size):\n    \"\"\"Builds a deeper, more regularized CNN-RNN model.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    # Make the CNN frontend deeper\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    # Reshape for the RNN\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((-1, new_feature_dim))(x)\n    \n    # Make the RNN backend deeper and with stronger dropout\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    # Output layer\n    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# --- 3. Model Definition and CTC Loss ---\n\n\"\"\"older model, has less layers but is proven to underfit given the data.\"\"\"\n'''\ndef build_model_old1(input_shape, vocab_size):\n    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n    '''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:37:34.239784Z","iopub.execute_input":"2025-10-24T05:37:34.240461Z","iopub.status.idle":"2025-10-24T05:37:34.254662Z","shell.execute_reply.started":"2025-10-24T05:37:34.240440Z","shell.execute_reply":"2025-10-24T05:37:34.253891Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\ndef build_model_old1(input_shape, vocab_size):\\n    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\\n\\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\\n    x = tf.keras.layers.BatchNormalization()(x)\\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\\n    \\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\\n    x = tf.keras.layers.BatchNormalization()(x)\\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\\n\\n    _, time_dim, freq_dim, channel_dim = x.shape\\n    new_feature_dim = freq_dim * channel_dim\\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\\n    \\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\\n    x = tf.keras.layers.BatchNormalization()(x)\\n    \\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\\n    x = tf.keras.layers.BatchNormalization()(x)\\n\\n    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\\n\\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\\n    return model\\n    '"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\ndef ctc_loss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n\n    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    \n    # Compute actual label lengths\n    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n    label_length = tf.cast(label_length, dtype=\"int64\")\n    #label_length = tf.minimum(label_length, input_length)\n    \n    loss = tf.keras.backend.ctc_batch_cost(\n        y_true,\n        y_pred,\n        input_length,\n        label_length,\n    )\n\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:37:43.513231Z","iopub.execute_input":"2025-10-24T05:37:43.513528Z","iopub.status.idle":"2025-10-24T05:37:43.518685Z","shell.execute_reply.started":"2025-10-24T05:37:43.513507Z","shell.execute_reply":"2025-10-24T05:37:43.517707Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# --- 4. Main Training Logic ---\n\nif __name__ == \"__main__\":\n    # load the dataset\n    paths, labels = load_data()\n    \n    # Split data\n    split_idx = int(len(paths) * 0.9)\n    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n    \n    # Build data pipelines\n    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n    \n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    total_decay_steps = steps_per_epoch * EPOCHS\n\n    #Note:you could use cosine_schedule for the optimizer, which would likely have better convergence over larger epochs , but reducelr is faster.\n    #IMP: DO NOT USE BOTH cosine_schedule AND reduce_lr, AS THIS LEADS TO UNPREDICTABLE BEHAVIOUR.\n    '''\n    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-2,  # The starting learning rate\n        decay_steps=total_decay_steps, # The number of steps to decay over\n        alpha=0 # The minimum learning rate as a fraction of the initial rate\n    )\n    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n    '''\n    # Build the model\n    # We don't know the exact input shape, so we use None for the time dimension\n    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n    model.compile(optimizer=\"adam\", loss=ctc_loss)\n    model.summary()\n    model = tf.keras.models.load_model(\"/kaggle/working/kaggle/working/asr_model_best.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n    for x_batch, y_batch in train_ds.take(1):\n        preds = model(x_batch)\n        print(\"Model output time steps:\", preds.shape[1])\n        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n\n    # Set up callbacks\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"kaggle/working/asr_model_best.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n        verbose=1\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=1,\n    min_lr=1e-6,\n    verbose=1\n    )\n    \n    # Run some basic checks on the dataset  size before running the model\n    print(\"-\" * 20)\n    print(f\"Number of training file paths: {len(train_paths)}\")\n    estimated_steps = len(train_paths) // BATCH_SIZE\n    print(f\"Calculated steps_per_epoch should be around: {estimated_steps}\")\n    print(f\"Checking reported dataset cardinality...\")\n    reported_cardinality = train_ds.cardinality()\n    print(f\"train_ds.cardinality() reports: {reported_cardinality.numpy()}\")\n    print(\"-\" * 20)\n\n\n    \n    print(\"\\n--- Starting Model Training ---\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        initial_epoch=12,\n        callbacks=[model_checkpoint,reduce_lr]\n    )\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:42:39.825917Z","iopub.execute_input":"2025-10-24T05:42:39.826436Z","iopub.status.idle":"2025-10-24T06:26:54.947113Z","shell.execute_reply.started":"2025-10-24T05:42:39.826409Z","shell.execute_reply":"2025-10-24T06:26:54.946513Z"}},"outputs":[{"name":"stdout","text":"585\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_spectrogram (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_19 (\u001b[38;5;33mCast\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout2d_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_35          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout2d_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_4 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_16                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,147,776\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_36          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_17                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_37          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_18                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_38          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_19                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_39          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │        \u001b[38;5;34m15,390\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_spectrogram (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout2d_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_35          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout2d_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_16                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,147,776</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_36          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_17                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_37          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_18                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_38          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_19                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_39          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,961,854\u001b[0m (30.37 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,961,854</span> (30.37 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,957,374\u001b[0m (30.35 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,957,374</span> (30.35 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,480\u001b[0m (17.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> (17.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model output time steps: 402\nMax label length in batch: tf.Tensor(260, shape=(), dtype=int64)\n--------------------\nNumber of training file paths: 25685\nCalculated steps_per_epoch should be around: 802\nChecking reported dataset cardinality...\ntrain_ds.cardinality() reports: 803\n--------------------\n\n--- Starting Model Training ---\nEpoch 13/15\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 110.3660\nEpoch 13: val_loss improved from inf to 110.35689, saving model to kaggle/working/asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1325s\u001b[0m 2s/step - loss: 110.3648 - val_loss: 110.3569 - learning_rate: 5.0000e-04\nEpoch 14/15\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797ms/step - loss: 105.9582\nEpoch 14: val_loss improved from 110.35689 to 109.17303, saving model to kaggle/working/asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 836ms/step - loss: 105.9585 - val_loss: 109.1730 - learning_rate: 5.0000e-04\nEpoch 15/15\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777ms/step - loss: 105.5354\nEpoch 15: val_loss improved from 109.17303 to 107.46040, saving model to kaggle/working/asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 815ms/step - loss: 105.5345 - val_loss: 107.4604 - learning_rate: 5.0000e-04\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    # Save the final model\n    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n    print(\"\\n--- Training Part 1 complete. Final model saved as asr_model_final_ep15.keras ---\")\n    print(\"Best performing model during training saved as asr_model_best.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\nmodel = tf.keras.models.load_model(\"/kaggle/working/asr_model_best.keras\", custom_objects={\"ctc_loss\": ctc_loss})\nmodel.optimizer.learning_rate.assign(1e-5)\n\n# Recreate the same callbacks\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=3,\n    min_lr=1e-7,\n    verbose=1\n)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"asr_model_best.keras\",\n    save_best_only=True,\n    monitor=\"val_loss\",\n    verbose=1\n)\n\n# Resume training from epoch 15 → 25\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=25,\n    initial_epoch=15,\n    callbacks=[model_checkpoint, reduce_lr]\n)\n\nmodel.save(\"/kaggle/working/asr_model_final_ep25.keras\")\nprint(\"\\n--- Training Part 2 complete. Final model saved as asr_model_final_ep15.keras ---\")\nprint(\"Best performing model during training saved as asr_model_best.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:53:51.944984Z","iopub.execute_input":"2025-10-24T06:53:51.945699Z","iopub.status.idle":"2025-10-24T08:41:43.516342Z","shell.execute_reply.started":"2025-10-24T06:53:51.945670Z","shell.execute_reply":"2025-10-24T08:41:43.515583Z"}},"outputs":[{"name":"stdout","text":"Epoch 16/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - loss: 99.4657\nEpoch 16: val_loss improved from inf to 104.98033, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 810ms/step - loss: 99.4642 - val_loss: 104.9803 - learning_rate: 1.0000e-05\nEpoch 17/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767ms/step - loss: 98.5005\nEpoch 17: val_loss improved from 104.98033 to 104.60508, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 806ms/step - loss: 98.4992 - val_loss: 104.6051 - learning_rate: 1.0000e-05\nEpoch 18/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771ms/step - loss: 97.6072\nEpoch 18: val_loss improved from 104.60508 to 104.36993, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 810ms/step - loss: 97.6063 - val_loss: 104.3699 - learning_rate: 1.0000e-05\nEpoch 19/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - loss: 97.1505\nEpoch 19: val_loss improved from 104.36993 to 104.04418, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 799ms/step - loss: 97.1495 - val_loss: 104.0442 - learning_rate: 1.0000e-05\nEpoch 20/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767ms/step - loss: 96.9447\nEpoch 20: val_loss improved from 104.04418 to 103.87014, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 805ms/step - loss: 96.9435 - val_loss: 103.8701 - learning_rate: 1.0000e-05\nEpoch 21/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765ms/step - loss: 95.7571\nEpoch 21: val_loss improved from 103.87014 to 103.65413, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 804ms/step - loss: 95.7567 - val_loss: 103.6541 - learning_rate: 1.0000e-05\nEpoch 22/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765ms/step - loss: 96.0503\nEpoch 22: val_loss improved from 103.65413 to 103.34695, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 804ms/step - loss: 96.0490 - val_loss: 103.3469 - learning_rate: 1.0000e-05\nEpoch 23/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766ms/step - loss: 95.7289\nEpoch 23: val_loss did not improve from 103.34695\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 804ms/step - loss: 95.7279 - val_loss: 103.3787 - learning_rate: 1.0000e-05\nEpoch 24/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762ms/step - loss: 95.1557\nEpoch 24: val_loss did not improve from 103.34695\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 799ms/step - loss: 95.1550 - val_loss: 103.3807 - learning_rate: 1.0000e-05\nEpoch 25/25\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766ms/step - loss: 94.6592\nEpoch 25: val_loss improved from 103.34695 to 103.19284, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 804ms/step - loss: 94.6588 - val_loss: 103.1928 - learning_rate: 1.0000e-05\n\n--- Training Part 2 complete. Final model saved as asr_model_final_ep15.keras ---\nBest performing model during training saved as asr_model_best.keras\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"\nmodel = tf.keras.models.load_model(\"/kaggle/working/asr_model_best.keras\", custom_objects={\"ctc_loss\": ctc_loss})\nmodel.optimizer.learning_rate.assign(1e-3)\n\n# Recreate the same callbacks\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    min_lr=1e-7,\n    verbose=1\n)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"asr_model_best.keras\",\n    save_best_only=True,\n    monitor=\"val_loss\",\n    verbose=1\n)\n\n# Resume training from epoch 25 → 35\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=35,\n    initial_epoch=25,\n    callbacks=[model_checkpoint, reduce_lr]\n)\n\nmodel.save(\"/kaggle/working/asr_model_final_ep35.keras\")\nprint(\"\\n--- Training Part 3 complete. Final model saved as asr_model_final_ep15.keras ---\")\nprint(\"Best performing model during training saved as asr_model_best.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:48:06.666211Z","iopub.execute_input":"2025-10-24T08:48:06.666894Z","iopub.status.idle":"2025-10-24T09:38:48.398082Z","shell.execute_reply.started":"2025-10-24T08:48:06.666866Z","shell.execute_reply":"2025-10-24T09:38:48.396938Z"}},"outputs":[{"name":"stdout","text":"Epoch 26/35\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - loss: 106.4837\nEpoch 26: val_loss improved from inf to 113.00412, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 801ms/step - loss: 106.4878 - val_loss: 113.0041 - learning_rate: 0.0010\nEpoch 27/35\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765ms/step - loss: 108.2911\nEpoch 27: val_loss improved from 113.00412 to 112.75507, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 804ms/step - loss: 108.2913 - val_loss: 112.7551 - learning_rate: 0.0010\nEpoch 28/35\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762ms/step - loss: 106.2166\nEpoch 28: val_loss improved from 112.75507 to 108.78432, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 801ms/step - loss: 106.2176 - val_loss: 108.7843 - learning_rate: 0.0010\nEpoch 29/35\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760ms/step - loss: 103.8715\nEpoch 29: val_loss improved from 108.78432 to 108.54433, saving model to asr_model_best.keras\n\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 799ms/step - loss: 103.8721 - val_loss: 108.5443 - learning_rate: 0.0010\nEpoch 30/35\n\u001b[1m599/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 764ms/step - loss: 103.5455","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_549/1987968199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Resume training from epoch 25 → 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":25}]}