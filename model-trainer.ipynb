{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from audiomentations import AddGaussianNoise\n",
    "import random\n",
    "from scipy.signal import butter, lfilter\n",
    "from IPython.display import Audio\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Augmenter:\n",
    "\n",
    "    def __init__(self, sr=16000,\n",
    "                 noise_prob=0.4, noise_max_amp=0.01,\n",
    "                 reverb_prob=0.3, reverb_delay=0.025, reverb_decay=0.2,\n",
    "                 shuffle_prob=0.05, shuffle_segments=3,\n",
    "                 time_stretch_prob=0.2, time_stretch_range=(0.9, 1.1),\n",
    "                 gaps_prob=0.08, gaps_n=4, gaps_max_duration=0.1,\n",
    "                 freq_mask_prob=0.2, freq_mask_n=1,\n",
    "                 shuffle_seg_dur=0.08, shuffle_overlap=0.02, shuffle_local_range=3):\n",
    "\n",
    "        self.sr = sr\n",
    "        self.noise_aug = AddGaussianNoise(p=1.0, max_amplitude=noise_max_amp)\n",
    "\n",
    "        self.noise_prob = noise_prob\n",
    "        self.reverb_prob = reverb_prob\n",
    "        self.reverb_delay = reverb_delay\n",
    "        self.reverb_decay = reverb_decay\n",
    "        self.shuffle_prob = shuffle_prob\n",
    "        self.shuffle_segments = shuffle_segments\n",
    "        self.time_stretch_prob = time_stretch_prob\n",
    "        self.time_stretch_range = time_stretch_range\n",
    "        self.gaps_prob = gaps_prob\n",
    "        self.gaps_n = gaps_n\n",
    "        self.gaps_max_duration = gaps_max_duration\n",
    "        self.freq_mask_prob = freq_mask_prob\n",
    "        self.freq_mask_n = freq_mask_n\n",
    "        self.shuffle_seg_dur = shuffle_seg_dur\n",
    "        self.shuffle_overlap = shuffle_overlap\n",
    "        self.shuffle_local_range = shuffle_local_range\n",
    "\n",
    "    def augment(self, audio):\n",
    "        if not isinstance(audio, tf.Tensor):\n",
    "            audio = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "        #Set of distortions to be applied randomly with probabilities below\n",
    "        distortions = []\n",
    "\n",
    "        # 1. Noise\n",
    "        # Adds gaussian noise -> makes it slightly grainy\n",
    "        # Min max amplitude of noise - not set\n",
    "        # p=1.0 - always apply noise\n",
    "        if random.random() < self.noise_prob:\n",
    "            distortions.append('noise')\n",
    "\n",
    "        # 2. Reverb\n",
    "        # Echo effect\n",
    "        # Delay the audio by 0.1 -> reduce volume -> pad it to original length -> add\n",
    "        if random.random() < self.reverb_prob:\n",
    "            distortions.append('reverb')\n",
    "\n",
    "        # 3. Shuffle\n",
    "        # Break into n segments and concat them randomly\n",
    "        if random.random() < self.shuffle_prob:\n",
    "            distortions.append('shuffle')\n",
    "\n",
    "        # 4. Time stretch\n",
    "        # Randomly slows (0.9) or speeds (1.1) the audio / doesn't change pitch\n",
    "        if random.random() < self.time_stretch_prob:\n",
    "            distortions.append('time_stretch')\n",
    "\n",
    "        # 5. Missing Gaps\n",
    "        # Randomly insert silences/gaps in the audio\n",
    "        if random.random() < self.gaps_prob:\n",
    "            distortions.append('missing_gaps')\n",
    "\n",
    "        # 6. Frequency Masking\n",
    "        # Randomly masks a range of frequencies in the spectrogram\n",
    "        #Butterworth filter is better than applying freqeuncy masks on spectogram (which already has frequency bins) because real wrld freq loss occurs during sound capture/transmission, affecting the raw audio.\n",
    "        #Butterworth simulates this situation by removing frequency content from the waveform which can then go thru the rest of pipeline,\n",
    "        #additionally it affects the phase relations and harmonics naturally in contrast to the crude zeroing of freq bins in spectogram\n",
    "\n",
    "        if random.random() < self.freq_mask_prob:\n",
    "            distortions.append('frequency_masking')\n",
    "\n",
    "\n",
    "\n",
    "        # Apply selected distortions\n",
    "        for distortion in distortions:\n",
    "            if distortion == 'noise':\n",
    "                audio = self._add_noise(audio)\n",
    "\n",
    "            elif distortion == 'reverb':\n",
    "                audio = self._add_reverb(audio)\n",
    "\n",
    "            elif distortion == 'shuffle':\n",
    "                audio = self._segment_shuffle(audio)\n",
    "\n",
    "            elif distortion == 'time_stretch':\n",
    "                audio = self._time_stretch(audio)\n",
    "\n",
    "            elif distortion == 'missing_gaps':\n",
    "                audio = self._add_missing_gaps(audio)\n",
    "\n",
    "            elif distortion == 'frequency_masking':\n",
    "                audio = self._add_frequency_mask(audio)\n",
    "\n",
    "        return audio\n",
    "    def _add_noise(self, audio):\n",
    "        return self.noise_aug(audio, self.sr)\n",
    "\n",
    "    def _add_reverb(self, audio):\n",
    "        delay = int(self.reverb_delay * self.sr)  # Delay in samples (0.05 sec)\n",
    "        reverb = tf.pad(audio * self.reverb_decay, [[delay, 0]]) #Amplitude scaling -> 0.2\n",
    "        reverb = reverb[:tf.shape(audio)[0]]\n",
    "        return audio + reverb\n",
    "\n",
    "    def _segment_shuffle(self, audio, n_segments=None):\n",
    "        # Previous method -> shuffle random large segments / unrealistic and destroys linguistic stuff\n",
    "        # if n_segments is None:\n",
    "        #     n_segments = self.shuffle_segments\n",
    "        # segments = np.array_split(audio, n_segments)\n",
    "        # np.random.shuffle(segments)\n",
    "        # return np.concatenate(segments)\n",
    "\n",
    "\n",
    "        #New method -> try to simulate temporal jitter / noise in the time domain -> split into micro-segments which are overlapping  -> the segments are shuffle locally within shuffle range\n",
    "        seg_len = int(self.shuffle_seg_dur * self.sr)\n",
    "        overlap = int(self.shuffle_overlap * self.sr)\n",
    "        local_range = self.shuffle_local_range\n",
    "        segments = []\n",
    "        i = 0\n",
    "        while i < len(audio):\n",
    "            end = min(i + seg_len, len(audio))\n",
    "            segments.append(audio[i:end])\n",
    "            i += seg_len - overlap\n",
    "        n_regions = min(4, max(1, len(segments) // 10))\n",
    "        region_indices = random.sample(range(len(segments)), n_regions)\n",
    "        shuffled = segments.copy()\n",
    "        for r in region_indices:\n",
    "            for offset in range(-local_range, local_range + 1):\n",
    "                idx = r + offset\n",
    "                if 0 <= idx < len(segments):\n",
    "                    shift = random.randint(-local_range, local_range)\n",
    "                    new_idx = max(0, min(len(segments) - 1, idx + shift))\n",
    "                    shuffled[idx] = segments[new_idx]\n",
    "        return np.concatenate(shuffled)[:len(audio)]\n",
    "\n",
    "\n",
    "    def _time_stretch(self, audio):\n",
    "        return tf.constant(librosa.effects.time_stretch(audio, rate=random.uniform(*self.time_stretch_range)),dtype = tf.float32)\n",
    "\n",
    "    def _add_missing_gaps(self, audio, n_gaps=None, max_gap_duration=None):\n",
    "        # if n_gaps is None:\n",
    "        #     n_gaps = self.gaps_n\n",
    "        # if max_gap_duration is None:\n",
    "        #     max_gap_duration = self.gaps_max_duration\n",
    "        # gap_audio = np.copy(audio)\n",
    "        # for _ in range(n_gaps):\n",
    "        #     gap_duration = random.uniform(0.1, max_gap_duration)\n",
    "        #     gap_samples = int(gap_duration * self.sr)\n",
    "        #     start = random.randint(0, max(1, len(audio) - gap_samples))\n",
    "        #     gap_audio[start:start + gap_samples] = 0\n",
    "        # return gap_audio\n",
    "\n",
    "        #New method -> fill gaps with low level noises and make edges smoother\n",
    "        if n_gaps is None:\n",
    "            n_gaps = self.gaps_n\n",
    "        if max_gap_duration is None:\n",
    "            max_gap_duration = self.gaps_max_duration\n",
    "\n",
    "        gap_audio = tf.identity(audio)\n",
    "\n",
    "        for _ in range(n_gaps):\n",
    "            gap_duration = tf.random.uniform([], 0.1, max_gap_duration, dtype=tf.float32)\n",
    "            gap_samples = tf.cast(gap_duration * tf.cast(self.sr, tf.float32), tf.int32)\n",
    "            start = tf.random.uniform([], 0, tf.shape(audio)[0] - gap_samples, dtype=tf.int32)\n",
    "\n",
    "            fade_len = tf.minimum(tf.cast(0.05 * tf.cast(gap_samples, tf.float32), tf.int32), gap_samples // 4)\n",
    "            fade_out = tf.cast(tf.linspace(1.0, 0.0, fade_len), tf.float32)\n",
    "            fade_in = tf.cast(tf.linspace(0.0, 1.0, fade_len), tf.float32)\n",
    "\n",
    "            mid_len = gap_samples - 2 * fade_len\n",
    "\n",
    "            if mid_len > 0:\n",
    "            #In case 0 values are needed.\n",
    "            #     if random.random() < 0.5:\n",
    "            #         gap_audio[start + fade_len:start + fade_len + mid_len] = 0\n",
    "            #     else:\n",
    "              noise = tf.random.normal([mid_len], stddev=0.001, dtype=tf.float32)\n",
    "              indices = tf.reshape(tf.range(start + fade_len, start + fade_len + mid_len), (-1, 1))\n",
    "              gap_audio = tf.tensor_scatter_nd_update(gap_audio, indices, noise)\n",
    "\n",
    "        fade_indices = tf.reshape(tf.range(start + fade_len + mid_len, start + gap_samples), (-1, 1))\n",
    "        fade_vals = gap_audio[start + fade_len + mid_len:start + gap_samples] * fade_in\n",
    "        gap_audio = tf.tensor_scatter_nd_update(gap_audio, fade_indices, fade_vals)\n",
    "\n",
    "        return gap_audio[:len(audio)]\n",
    "    #APPLY FREQUENCY MASKING USING BUTTERWORTH FILTER\n",
    "    #n_masks -> how many bands will be filtered out\n",
    "    #A 16 kHz sampler can only capture up to 8 kHz frequencies because you need at least two samples per wave cycle to know what the wave looks like -> nyquist\n",
    "    def _add_frequency_mask(self, audio, n_masks=None):\n",
    "      if n_masks is None:\n",
    "          n_masks = self.freq_mask_n\n",
    "      if isinstance(audio, tf.Tensor):\n",
    "        masked_audio = audio.numpy().copy()\n",
    "      else:\n",
    "        masked_audio = audio.copy()\n",
    "      nyquist = self.sr/2\n",
    "      for _ in range(n_masks):\n",
    "        l_freq = random.uniform(500,5000)\n",
    "        h_freq = l_freq + random.uniform(500,2000)\n",
    "        l_freq = min(l_freq,nyquist-100)\n",
    "        h_freq = min(h_freq,nyquist-100)\n",
    "        b,a = butter(N=4, Wn=[l_freq,h_freq], btype= \"bandstop\", fs = self.sr) #N=4 th order -> a dip in frequency response where that removal band is with smooth edges\n",
    "        masked_audio = lfilter(b,a,masked_audio)\n",
    "      return masked_audio\n",
    "augmenter=Augmenter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transfer and conversion from './dataset/LibriSpeech/train-clean-100/'...\n",
      "Output will be saved in './dataset/LibriSpeech-WAV-Complete/'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     fd \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mopen(file, flags, \u001b[38;5;241m0o600\u001b[39m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'c:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\librosa\\\\core\\\\__pycache__\\\\tmpydd2fi4u'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m wav_filename \u001b[38;5;241m=\u001b[39m Path(filename)\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, wav_filename)\n\u001b[1;32m---> 32\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(input_file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Apply your custom augmentations\u001b[39;00m\n\u001b[0;32m     34\u001b[0m augmented_audio \u001b[38;5;241m=\u001b[39m augmenter\u001b[38;5;241m.\u001b[39maugment(audio)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\lazy_loader\\__init__.py:83\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     81\u001b[0m submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m submod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(submod_path)\n\u001b[1;32m---> 83\u001b[0m attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m attr_to_modules[name]:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\lazy_loader\\__init__.py:82\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m     81\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     submod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(submod_path)\n\u001b[0;32m     83\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1026\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jit, stencil, guvectorize\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_fftlib\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frames_to_samples, time_to_samples\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\convert.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notation\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParameterError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vectorize\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\notation.py:796\u001b[0m\n\u001b[0;32m    789\u001b[0m     acc_str \u001b[38;5;241m=\u001b[39m acc_map_inv[np\u001b[38;5;241m.\u001b[39msign(acc_index) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28mabs\u001b[39m(acc_index) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    791\u001b[0m     ) \u001b[38;5;241m+\u001b[39m acc_map_inv[np\u001b[38;5;241m.\u001b[39msign(acc_index)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mabs\u001b[39m(acc_index) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_output \u001b[38;5;241m+\u001b[39m acc_str\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;129m@jit\u001b[39m(nopython\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nogil\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__o_fold\u001b[39m(d):\n\u001b[0;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the octave-folded interval.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m    This maps intervals to the range [1, 2).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    documentation.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloor(np\u001b[38;5;241m.\u001b[39mlog2(d)))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\decorators.py:225\u001b[0m, in \u001b[0;36m_jit.<locals>.wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    221\u001b[0m disp \u001b[38;5;241m=\u001b[39m dispatcher(py_func\u001b[38;5;241m=\u001b[39mfunc, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m,\n\u001b[0;32m    222\u001b[0m                   targetoptions\u001b[38;5;241m=\u001b[39mtargetoptions,\n\u001b[0;32m    223\u001b[0m                   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdispatcher_args)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache:\n\u001b[1;32m--> 225\u001b[0m     disp\u001b[38;5;241m.\u001b[39menable_caching()\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sigs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Register the Dispatcher to the type inference mechanism,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# even though the decorator hasn't returned yet.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m typeinfer\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\dispatcher.py:807\u001b[0m, in \u001b[0;36mDispatcher.enable_caching\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable_caching\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m FunctionCache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_func)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:647\u001b[0m, in \u001b[0;36mCache.__init__\u001b[1;34m(self, py_func)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(py_func)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_func \u001b[38;5;241m=\u001b[39m py_func\n\u001b[1;32m--> 647\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_class(py_func)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl\u001b[38;5;241m.\u001b[39mlocator\u001b[38;5;241m.\u001b[39mget_cache_path()\n\u001b[0;32m    649\u001b[0m \u001b[38;5;66;03m# This may be a bit strict but avoids us maintaining a magic number\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:379\u001b[0m, in \u001b[0;36mCacheImpl.__init__\u001b[1;34m(self, py_func)\u001b[0m\n\u001b[0;32m    377\u001b[0m source_path \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetfile(py_func)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locator_classes:\n\u001b[1;32m--> 379\u001b[0m     locator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_function(py_func, source_path)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m locator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:182\u001b[0m, in \u001b[0;36m_SourceFileBackedLocatorMixin.from_function\u001b[1;34m(cls, py_func, py_file)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(py_func, py_file)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensure_cache_path()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Cannot ensure the cache directory exists or is writable\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:109\u001b[0m, in \u001b[0;36m_CacheLocator.ensure_cache_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Ensure the directory is writable by trying to write a temporary file\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m tempfile\u001b[38;5;241m.\u001b[39mTemporaryFile(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mpath)\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\tempfile.py:582\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors, delete_on_close)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     file \u001b[38;5;241m=\u001b[39m _io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mdir\u001b[39m, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[0;32m    583\u001b[0m                     newline\u001b[38;5;241m=\u001b[39mnewline, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    584\u001b[0m                     opener\u001b[38;5;241m=\u001b[39mopener)\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer\u001b[39m\u001b[38;5;124m'\u001b[39m, file)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\tempfile.py:579\u001b[0m, in \u001b[0;36mNamedTemporaryFile.<locals>.opener\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopener\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m name\n\u001b[1;32m--> 579\u001b[0m     fd, name \u001b[38;5;241m=\u001b[39m _mkstemp_inner(\u001b[38;5;28mdir\u001b[39m, prefix, suffix, flags, output_type)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\tempfile.py:262\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# This exception is thrown when a directory with the chosen name\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# already exists on windows.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (_os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mdir\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    263\u001b[0m         _os\u001b[38;5;241m.\u001b[39maccess(\u001b[38;5;28mdir\u001b[39m, _os\u001b[38;5;241m.\u001b[39mW_OK)):\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# 1. Set the path to your main dataset directory\n",
    "input_root_dir = \"./dataset/LibriSpeech/train-clean-100/\"\n",
    "\n",
    "# 2. Set the path where you want to save the new dataset\n",
    "output_root_dir = \"./dataset/LibriSpeech-WAV-Complete/\"\n",
    "# ---\n",
    "\n",
    "augmenter=Augmenter()\n",
    "print(f\"Starting transfer and conversion from '{input_root_dir}'...\")\n",
    "print(f\"Output will be saved in '{output_root_dir}'.\")\n",
    "\n",
    "# Walk through the entire directory structure\n",
    "for dirpath, _, filenames in os.walk(input_root_dir):\n",
    "    for filename in filenames:\n",
    "        # Construct the full path to the source file\n",
    "        input_file_path = os.path.join(dirpath, filename)\n",
    "        \n",
    "        # Create the corresponding output directory structure\n",
    "        relative_path = os.path.relpath(dirpath, input_root_dir)\n",
    "        output_dir = os.path.join(output_root_dir, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # --- Logic to either convert or copy the file ---\n",
    "        try:\n",
    "            if filename.endswith(\".flac\"):\n",
    "                # It's an audio file, so convert it to WAV\n",
    "\n",
    "                # Create the full path for the output WAV file\n",
    "                wav_filename = Path(filename).stem + \".wav\"\n",
    "                output_file_path = os.path.join(output_dir, wav_filename)\n",
    "                audio, sr = librosa.load(input_file_path, sr=16000)\n",
    "                # Apply your custom augmentations\n",
    "                augmented_audio = augmenter.augment(audio)\n",
    "                sf.write(output_file_path, augmented_audio, sr)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                # It's a non-audio file (e.g., .txt), so copy it directly\n",
    "                \n",
    "                # Construct the output path\n",
    "                output_file_path = os.path.join(output_dir, filename)\n",
    "                \n",
    "                # Use shutil.copy2 to preserve file metadata\n",
    "                shutil.copy2(input_file_path, output_file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"New dataset is ready at: '{output_root_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.690225Z",
     "iopub.status.busy": "2025-10-19T19:06:28.689528Z",
     "iopub.status.idle": "2025-10-19T19:06:28.711751Z",
     "shell.execute_reply": "2025-10-19T19:06:28.710983Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.690200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "Training_dirs=\"./dataset/LibriSpeech-WAV-Complete/\"\n",
    "# Parameters\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 # Set to a higher number for real training\n",
    "\n",
    "# Create a dummy vocabulary with only uppercase letters and apostrophe\n",
    "CHARACTERS = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    \"'\",' ']\n",
    "\n",
    "# Create character-to-number mappings\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\n",
    "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "VOCAB_SIZE = char_to_num.vocabulary_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.714011Z",
     "iopub.status.busy": "2025-10-19T19:06:28.713298Z",
     "iopub.status.idle": "2025-10-19T19:06:28.720483Z",
     "shell.execute_reply": "2025-10-19T19:06:28.719826Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.713980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to create dummy data\n",
    "def load_data():\n",
    "    file_paths = []\n",
    "    transcriptions = []\n",
    "    directories=[]\n",
    "    label_files=[]\n",
    "    for lfold1 in os.listdir(Training_dirs):\n",
    "        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n",
    "            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(full_path)\n",
    "                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n",
    "    for label_path in label_files:\n",
    "        with open(label_path,'r') as labels:\n",
    "            for line in labels.readlines():\n",
    "                transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n",
    "    for path in directories:\n",
    "        fp=[]\n",
    "        for file in os.listdir(path):\n",
    "            if(file.endswith('.wav')):\n",
    "                fp.append(os.path.join(path,file))\n",
    "        fp.sort()\n",
    "        file_paths+=fp\n",
    "    \n",
    "    print(len(label_files))\n",
    "        \n",
    "    return file_paths, transcriptions\n",
    "#load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.721444Z",
     "iopub.status.busy": "2025-10-19T19:06:28.721198Z",
     "iopub.status.idle": "2025-10-19T19:06:28.744372Z",
     "shell.execute_reply": "2025-10-19T19:06:28.743708Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.721426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. tf.data Pipeline (No Augmentation) ---\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n",
    "    try:\n",
    "        path_str = file_path.numpy().decode('utf-8')\n",
    "        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Compute the Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
    "        \n",
    "        # Convert to log scale (decibels)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n",
    "        \n",
    "        return log_mel_spec.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path.numpy()}: {e}\")\n",
    "        os.exit()\n",
    "        return np.zeros((100, 80, 1), dtype=np.float32)\n",
    "       \n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 80\n",
    "\n",
    "def power_to_db(S, ref=1.0, top_db=80.0):\n",
    "    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n",
    "    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n",
    "    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n",
    "    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "\n",
    "@tf.function\n",
    "def preprocess_audio_tf(file_path: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n",
    "    with padding to match librosa's default behavior.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "    \n",
    "        # decode_wav returns a normalized float32 tensor and the sample rate.\n",
    "        # desired_channels=1 ensures the audio is mono.\n",
    "        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n",
    "    \n",
    "        # Squeeze the channel dimension, leaving a 1D waveform.\n",
    "        # NO further normalization is needed.\n",
    "        waveform = tf.squeeze(audio_tensor, axis=-1)\n",
    "\n",
    "        # --- FIX: Manually pad the waveform to match librosa ---\n",
    "        # (The rest of your function remains the same and is correct)\n",
    "        padding = N_FFT // 2\n",
    "        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n",
    "        \n",
    "        # --- 2. Compute the STFT (The rest is the same) ---\n",
    "        stft = tf.signal.stft(\n",
    "            waveform,\n",
    "            frame_length=N_FFT,\n",
    "            frame_step=HOP_LENGTH,\n",
    "            fft_length=N_FFT\n",
    "        )\n",
    "        spectrogram = tf.abs(stft)\n",
    "\n",
    "        # ... (rest of the function is identical) ...\n",
    "        power_spectrogram = spectrogram ** 2\n",
    "        num_spectrogram_bins = stft.shape[-1]\n",
    "        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=N_MELS,\n",
    "            num_spectrogram_bins=num_spectrogram_bins,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            lower_edge_hertz=20.0,\n",
    "            upper_edge_hertz=8000.0\n",
    "        )\n",
    "        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n",
    "        log_mel_spectrogram = power_to_db(mel_spectrogram)\n",
    "        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n",
    "\n",
    "        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n",
    "        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n",
    "    \n",
    "def preprocess_label(text_label):\n",
    "    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n",
    "    # Convert all characters to uppercase to match the vocabulary\n",
    "    text_tensor = tf.strings.upper(text_label)\n",
    "    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n",
    "    return char_to_num(chars)\n",
    "# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n",
    "\n",
    "@tf.function\n",
    "def preprocess_and_filter(path, label):\n",
    "    \"\"\"\n",
    "    Applies full preprocessing to audio and text, and returns their lengths.\n",
    "    \"\"\"\n",
    "    # Process the audio file to get the final spectrogram\n",
    "    spectrogram = preprocess_audio_tf(path)\n",
    "    \n",
    "    # Process the text label to get the integer tokens\n",
    "    processed_label = preprocess_label(label)\n",
    "\n",
    "    # Get the number of time steps from the spectrogram\n",
    "    spectrogram_length = tf.shape(spectrogram)[0]\n",
    "    \n",
    "    # Get the number of characters/tokens from the label\n",
    "    label_length = tf.shape(processed_label)[0]\n",
    "\n",
    "    return spectrogram, processed_label, spectrogram_length, label_length\n",
    "#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.809637Z",
     "iopub.status.busy": "2025-10-19T19:06:28.808934Z",
     "iopub.status.idle": "2025-10-19T19:06:28.816166Z",
     "shell.execute_reply": "2025-10-19T19:06:28.815278Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.809613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # Map preprocessing functions\n",
    "    ds = ds.map(\n",
    "        lambda path, label: (\n",
    "            tf.py_function(preprocess_audio, [path], tf.float32),\n",
    "            preprocess_label(label)\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Batch and pad\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds'''\n",
    "\n",
    "def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # 1. Map the combined preprocessing and length calculation function\n",
    "    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 2. Filter out items where the spectrogram is shorter than the label\n",
    "    ds = ds.filter(\n",
    "        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n",
    "    )\n",
    "    \n",
    "    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n",
    "    ds = ds.map(\n",
    "        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # 4. Batch and pad as before\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:24:34.026671Z",
     "iopub.status.busy": "2025-10-19T19:24:34.026335Z",
     "iopub.status.idle": "2025-10-19T19:24:34.038987Z",
     "shell.execute_reply": "2025-10-19T19:24:34.038268Z",
     "shell.execute_reply.started": "2025-10-19T19:24:34.026647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, vocab_size):\n",
    "    \"\"\"Builds a deeper, more regularized CNN-RNN model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n",
    "\n",
    "    # Make the CNN frontend deeper\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "\n",
    "    # Reshape for the RNN\n",
    "    _, time_dim, freq_dim, channel_dim = x.shape\n",
    "    new_feature_dim = freq_dim * channel_dim\n",
    "    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n",
    "    \n",
    "    # Make the RNN backend deeper and with stronger dropout\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# --- 3. Model Definition and CTC Loss ---\n",
    "\n",
    "\"\"\"older model, has less layers but is proven to underfit given the data.\"\"\"\n",
    "def build_model_old1(input_shape, vocab_size):\n",
    "    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    _, time_dim, freq_dim, channel_dim = x.shape\n",
    "    new_feature_dim = freq_dim * channel_dim\n",
    "    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.835259Z",
     "iopub.status.busy": "2025-10-19T19:06:28.835041Z",
     "iopub.status.idle": "2025-10-19T19:06:28.851013Z",
     "shell.execute_reply": "2025-10-19T19:06:28.850375Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.835242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n",
    "    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    \n",
    "    # Compute actual label lengths\n",
    "    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n",
    "    label_length = tf.cast(label_length, dtype=\"int64\")\n",
    "    #label_length = tf.minimum(label_length, input_length)\n",
    "    \n",
    "    loss = tf.keras.backend.ctc_batch_cost(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        input_length,\n",
    "        label_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:24:40.639501Z",
     "iopub.status.busy": "2025-10-19T19:24:40.639197Z",
     "iopub.status.idle": "2025-10-19T21:44:49.866558Z",
     "shell.execute_reply": "2025-10-19T21:44:49.865857Z",
     "shell.execute_reply.started": "2025-10-19T19:24:40.639478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './dataset/LibriSpeech-WAV-Complete/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- 4. Main Training and Saving Logic ---\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Generate the dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     paths, labels \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Split data (simple split for demonstration)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     split_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m directories\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      6\u001b[0m label_files\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lfold1 \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(Training_dirs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lfold2 \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(Training_dirs,lfold1)):\n\u001b[0;32m      9\u001b[0m         full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(Training_dirs, lfold1,lfold2)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './dataset/LibriSpeech-WAV-Complete/'"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Training and Saving Logic ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate the dataset\n",
    "    paths, labels = load_data()\n",
    "    \n",
    "    # Split data (simple split for demonstration)\n",
    "    split_idx = int(len(paths) * 0.9)\n",
    "    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n",
    "    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # Build data pipelines\n",
    "    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n",
    "    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n",
    "    \n",
    "    steps_per_epoch = len(train_paths) // BATCH_SIZE\n",
    "    total_decay_steps = steps_per_epoch * EPOCHS\n",
    "    \n",
    "    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=1e-2,  # The starting learning rate\n",
    "        decay_steps=total_decay_steps, # The number of steps to decay over\n",
    "        alpha=0 # The minimum learning rate as a fraction of the initial rate\n",
    "    )\n",
    "    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n",
    "    # Build the model\n",
    "    # We don't know the exact input shape, so we use None for the time dimension\n",
    "    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n",
    "    model.compile(optimizer=\"adam\", loss=ctc_loss)\n",
    "    \n",
    "    model.summary()\n",
    "    for x_batch, y_batch in train_ds.take(1):\n",
    "        preds = model(x_batch)\n",
    "        print(\"Model output time steps:\", preds.shape[1])\n",
    "        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n",
    "\n",
    "    # Set up callbacks\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"asr_model_best.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1\n",
    "    )\n",
    "    # Train the model\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[model_checkpoint]\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T21:49:32.805784Z",
     "iopub.status.busy": "2025-10-19T21:49:32.805509Z",
     "iopub.status.idle": "2025-10-19T21:49:33.453078Z",
     "shell.execute_reply": "2025-10-19T21:49:33.452272Z",
     "shell.execute_reply.started": "2025-10-19T21:49:32.805765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training complete. Final model saved as asr_model_final.keras ---\n",
      "Best performing model during training saved as asr_model_best.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Save the final model\n",
    "    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n",
    "    print(\"\\n--- Training complete. Final model saved as asr_model_final.keras ---\")\n",
    "    print(\"Best performing model during training saved as asr_model_best.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-19T19:11:18.909098Z",
     "iopub.status.idle": "2025-10-19T19:11:18.909417Z",
     "shell.execute_reply": "2025-10-19T19:11:18.909262Z",
     "shell.execute_reply.started": "2025-10-19T19:11:18.909247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#quantised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\n",
    "model = tf.keras.models.load_model(\"/kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n",
    "\n",
    "# (Optionally) Lower LR manually before continuing\n",
    "tf.keras.backend.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "\n",
    "# Recreate the same callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"asr_model_best.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Resume training from epoch 15 → 25\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    initial_epoch=15,\n",
    "    callbacks=[model_checkpoint, reduce_lr]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
