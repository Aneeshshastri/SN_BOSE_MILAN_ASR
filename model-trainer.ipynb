{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13461226,"sourceType":"datasetVersion","datasetId":8544558}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:47.415504Z","iopub.execute_input":"2025-10-22T04:37:47.415857Z","iopub.status.idle":"2025-10-22T04:37:51.964052Z","shell.execute_reply.started":"2025-10-22T04:37:47.415835Z","shell.execute_reply":"2025-10-22T04:37:51.962646Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.4.0)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.3->librosa) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.3->librosa) (2024.2.0)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport os\nimport matplotlib.pyplot as plt\nimport shutil\nfrom pathlib import Path\nimport random\nfrom scipy.signal import butter, lfilter\nfrom IPython.display import Audio\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:51.966382Z","iopub.execute_input":"2025-10-22T04:37:51.966778Z","iopub.status.idle":"2025-10-22T04:37:51.974198Z","shell.execute_reply.started":"2025-10-22T04:37:51.966750Z","shell.execute_reply":"2025-10-22T04:37:51.972511Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\n# --- 1. Configuration ---\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nTraining_dirs=\"/kaggle/input/milan-audio-noisy/data_aug\"\n# Parameters\nSAMPLE_RATE = 16000\nBATCH_SIZE = 32\nEPOCHS = 15 # Set to a higher number for real training\n\n# Create a dummy vocabulary with only uppercase letters and apostrophe\nCHARACTERS = [\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    \"'\",' ']\n\n# Create character-to-number mappings\nchar_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\nnum_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\nVOCAB_SIZE = char_to_num.vocabulary_size()\n","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:51.975622Z","iopub.execute_input":"2025-10-22T04:37:51.975926Z","iopub.status.idle":"2025-10-22T04:37:52.011038Z","shell.execute_reply.started":"2025-10-22T04:37:51.975903Z","shell.execute_reply":"2025-10-22T04:37:52.009602Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Function to create dummy data\ndef load_data():\n    file_paths = []\n    transcriptions = []\n    directories=[]\n    label_files=[]\n    for lfold1 in os.listdir(Training_dirs):\n       if(lfold1!=\"6437\"):\n        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n            if os.path.isdir(full_path):\n                directories.append(full_path)\n                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n        for label_path in label_files:\n            with open(label_path,'r') as labels:\n                for line in labels.readlines():\n                    transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n        for path in directories:\n            fp=[]\n            for file in os.listdir(path):\n                if(file.endswith('.flac')):\n                    fp.append(os.path.join(path,file))\n            fp.sort()\n            file_paths+=fp\n    \n    print(len(label_files))\n        \n    return file_paths, transcriptions\n#print(os.listdir(Training_dirs))\n#load_data()","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.013763Z","iopub.execute_input":"2025-10-22T04:37:52.014886Z","iopub.status.idle":"2025-10-22T04:37:52.028563Z","shell.execute_reply.started":"2025-10-22T04:37:52.014850Z","shell.execute_reply":"2025-10-22T04:37:52.027254Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# --- 2. tf.data Pipeline (No Augmentation) ---\n\ndef preprocess_audio(file_path):\n    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n    try:\n        path_str = file_path.numpy().decode('utf-8')\n        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n        \n        # Compute the Mel spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n        \n        # Convert to log scale (decibels)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Add a channel dimension\n        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n        \n        return log_mel_spec.astype(np.float32)\n    except Exception as e:\n        print(f\"Error processing file {file_path.numpy()}: {e}\")\n        os.exit()\n        return np.zeros((100, 80, 1), dtype=np.float32)\n       \n\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nN_MELS = 80\n\ndef power_to_db(S, ref=1.0, top_db=80.0):\n    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n\n@tf.function\ndef preprocess_audio_tf(file_path: tf.Tensor):\n    \"\"\"\n    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n    with padding to match librosa's default behavior.\n    \"\"\"\n    try:\n        \n        audio_binary = tf.io.read_file(file_path)\n    \n        # decode_wav returns a normalized float32 tensor and the sample rate.\n        # desired_channels=1 ensures the audio is mono.\n        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n    \n        # Squeeze the channel dimension, leaving a 1D waveform.\n        # NO further normalization is needed.\n        waveform = tf.squeeze(audio_tensor, axis=-1)\n\n        # --- FIX: Manually pad the waveform to match librosa ---\n        # (The rest of your function remains the same and is correct)\n        padding = N_FFT // 2\n        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n        \n        # --- 2. Compute the STFT (The rest is the same) ---\n        stft = tf.signal.stft(\n            waveform,\n            frame_length=N_FFT,\n            frame_step=HOP_LENGTH,\n            fft_length=N_FFT\n        )\n        spectrogram = tf.abs(stft)\n\n        # ... (rest of the function is identical) ...\n        power_spectrogram = spectrogram ** 2\n        num_spectrogram_bins = stft.shape[-1]\n        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n            num_mel_bins=N_MELS,\n            num_spectrogram_bins=num_spectrogram_bins,\n            sample_rate=SAMPLE_RATE,\n            lower_edge_hertz=20.0,\n            upper_edge_hertz=8000.0\n        )\n        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n        log_mel_spectrogram = power_to_db(mel_spectrogram)\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n\n        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n\n    except Exception as e:\n        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n    \ndef preprocess_label(text_label):\n    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n    # Convert all characters to uppercase to match the vocabulary\n    text_tensor = tf.strings.upper(text_label)\n    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n    return char_to_num(chars)\n# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n\n@tf.function\ndef preprocess_and_filter(path, label):\n    \"\"\"\n    Applies full preprocessing to audio and text, and returns their lengths.\n    \"\"\"\n    # Process the audio file to get the final spectrogram\n    spectrogram = preprocess_audio(path)\n    \n    # Process the text label to get the integer tokens\n    processed_label = preprocess_label(label)\n\n    # Get the number of time steps from the spectrogram\n    spectrogram_length = tf.shape(spectrogram)[0]\n    \n    # Get the number of characters/tokens from the label\n    label_length = tf.shape(processed_label)[0]\n\n    return spectrogram, processed_label, spectrogram_length, label_length\n#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.030589Z","iopub.execute_input":"2025-10-22T04:37:52.030969Z","iopub.status.idle":"2025-10-22T04:37:52.050818Z","shell.execute_reply.started":"2025-10-22T04:37:52.030934Z","shell.execute_reply":"2025-10-22T04:37:52.049580Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"'''def build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=len(paths))\n    \n    # Map preprocessing functions\n    ds = ds.map(\n        lambda path, label: (\n            tf.py_function(preprocess_audio, [path], tf.float32),\n            preprocess_label(label)\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # Batch and pad\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds'''\n\ndef build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=len(paths))\n    \n    # 1. Map the combined preprocessing and length calculation function\n    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # 2. Filter out items where the spectrogram is shorter than the label\n    ds = ds.filter(\n        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n    )\n    \n    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n    ds = ds.map(\n        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # 4. Batch and pad as before\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.051856Z","iopub.execute_input":"2025-10-22T04:37:52.052121Z","iopub.status.idle":"2025-10-22T04:37:52.080823Z","shell.execute_reply.started":"2025-10-22T04:37:52.052098Z","shell.execute_reply":"2025-10-22T04:37:52.079644Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def build_model(input_shape, vocab_size):\n    \"\"\"Builds a deeper, more regularized CNN-RNN model.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    # Make the CNN frontend deeper\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n\n    # Reshape for the RNN\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n    \n    # Make the RNN backend deeper and with stronger dropout\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    # Output layer\n    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# --- 3. Model Definition and CTC Loss ---\n\n\"\"\"older model, has less layers but is proven to underfit given the data.\"\"\"\ndef build_model_old1(input_shape, vocab_size):\n    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.081965Z","iopub.execute_input":"2025-10-22T04:37:52.082352Z","iopub.status.idle":"2025-10-22T04:37:52.113000Z","shell.execute_reply.started":"2025-10-22T04:37:52.082321Z","shell.execute_reply":"2025-10-22T04:37:52.111323Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\ndef ctc_loss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n\n    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    \n    # Compute actual label lengths\n    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n    label_length = tf.cast(label_length, dtype=\"int64\")\n    #label_length = tf.minimum(label_length, input_length)\n    \n    loss = tf.keras.backend.ctc_batch_cost(\n        y_true,\n        y_pred,\n        input_length,\n        label_length,\n    )\n\n\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.114323Z","iopub.execute_input":"2025-10-22T04:37:52.114710Z","iopub.status.idle":"2025-10-22T04:37:52.137558Z","shell.execute_reply.started":"2025-10-22T04:37:52.114672Z","shell.execute_reply":"2025-10-22T04:37:52.136412Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# --- 4. Main Training and Saving Logic ---\n\nif __name__ == \"__main__\":\n    # Generate the dataset\n    paths, labels = load_data()\n    \n    # Split data (simple split for demonstration)\n    split_idx = int(len(paths) * 0.9)\n    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n    \n    # Build data pipelines\n    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n    \n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    total_decay_steps = steps_per_epoch * EPOCHS\n    \n    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-2,  # The starting learning rate\n        decay_steps=total_decay_steps, # The number of steps to decay over\n        alpha=0 # The minimum learning rate as a fraction of the initial rate\n    )\n    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n    # Build the model\n    # We don't know the exact input shape, so we use None for the time dimension\n    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n    model.compile(optimizer=\"adam\", loss=ctc_loss)\n    \n    model.summary()\n    for x_batch, y_batch in train_ds.take(1):\n        preds = model(x_batch)\n        print(\"Model output time steps:\", preds.shape[1])\n        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n\n    # Set up callbacks\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"asr_model_best.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n        verbose=1\n    )\n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=[model_checkpoint]\n    )\n   ","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.138809Z","iopub.execute_input":"2025-10-22T04:37:52.139271Z","iopub.status.idle":"2025-10-22T04:37:52.431339Z","shell.execute_reply.started":"2025-10-22T04:37:52.139239Z","shell.execute_reply":"2025-10-22T04:37:52.429516Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3816064862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Split data (simple split for demonstration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3037563636.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mlabel_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTraining_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfold1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlfold2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlfold1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlfold2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.trans.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mtranscriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/milan-audio-noisy/data_aug/1841/159771/1841-159771.trans.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/milan-audio-noisy/data_aug/1841/159771/1841-159771.trans.txt'","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    # Save the final model\n    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n    print(\"\\n--- Training complete. Final model saved as asr_model_final.keras ---\")\n    print(\"Best performing model during training saved as asr_model_best.keras\")","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.432026Z","iopub.status.idle":"2025-10-22T04:37:52.432394Z","shell.execute_reply.started":"2025-10-22T04:37:52.432218Z","shell.execute_reply":"2025-10-22T04:37:52.432238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quantised model","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.434348Z","iopub.status.idle":"2025-10-22T04:37:52.434826Z","shell.execute_reply.started":"2025-10-22T04:37:52.434614Z","shell.execute_reply":"2025-10-22T04:37:52.434635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\nmodel = tf.keras.models.load_model(\"/kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n\n# (Optionally) Lower LR manually before continuing\ntf.keras.backend.set_value(model.optimizer.learning_rate, 1e-4)\n\n# Recreate the same callbacks\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=3,\n    min_lr=1e-6,\n    verbose=1\n)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"asr_model_best.keras\",\n    save_best_only=True,\n    monitor=\"val_loss\",\n    verbose=1\n)\n\n# Resume training from epoch 15 â†’ 25\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=25,\n    initial_epoch=15,\n    callbacks=[model_checkpoint, reduce_lr]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:52.438935Z","iopub.status.idle":"2025-10-22T04:37:52.440091Z","shell.execute_reply.started":"2025-10-22T04:37:52.439476Z","shell.execute_reply":"2025-10-22T04:37:52.439773Z"}},"outputs":[],"execution_count":null}]}