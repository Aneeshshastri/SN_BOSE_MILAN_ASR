{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from audiomentations import AddGaussianNoise\n",
    "import random\n",
    "from scipy.signal import butter, lfilter\n",
    "from IPython.display import Audio\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Augmenter:\n",
    "\n",
    "    def __init__(self, sr=16000,\n",
    "                 noise_prob=0.4, noise_max_amp=0.01,\n",
    "                 reverb_prob=0.3, reverb_delay=0.025, reverb_decay=0.2,\n",
    "                 shuffle_prob=0.05, shuffle_segments=3,\n",
    "                 time_stretch_prob=0.2, time_stretch_range=(0.9, 1.1),\n",
    "                 gaps_prob=0.08, gaps_n=4, gaps_max_duration=0.1,\n",
    "                 freq_mask_prob=0.2, freq_mask_n=1,\n",
    "                 shuffle_seg_dur=0.08, shuffle_overlap=0.02, shuffle_local_range=3):\n",
    "\n",
    "        self.sr = sr\n",
    "        self.noise_aug = AddGaussianNoise(p=1.0, max_amplitude=noise_max_amp)\n",
    "\n",
    "        self.noise_prob = noise_prob\n",
    "        self.reverb_prob = reverb_prob\n",
    "        self.reverb_delay = reverb_delay\n",
    "        self.reverb_decay = reverb_decay\n",
    "        self.shuffle_prob = shuffle_prob\n",
    "        self.shuffle_segments = shuffle_segments\n",
    "        self.time_stretch_prob = time_stretch_prob\n",
    "        self.time_stretch_range = time_stretch_range\n",
    "        self.gaps_prob = gaps_prob\n",
    "        self.gaps_n = gaps_n\n",
    "        self.gaps_max_duration = gaps_max_duration\n",
    "        self.freq_mask_prob = freq_mask_prob\n",
    "        self.freq_mask_n = freq_mask_n\n",
    "        self.shuffle_seg_dur = shuffle_seg_dur\n",
    "        self.shuffle_overlap = shuffle_overlap\n",
    "        self.shuffle_local_range = shuffle_local_range\n",
    "\n",
    "    def augment(self, audio):\n",
    "        if not isinstance(audio, tf.Tensor):\n",
    "            audio = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "        #Set of distortions to be applied randomly with probabilities below\n",
    "        distortions = []\n",
    "\n",
    "        # 1. Noise\n",
    "        # Adds gaussian noise -> makes it slightly grainy\n",
    "        # Min max amplitude of noise - not set\n",
    "        # p=1.0 - always apply noise\n",
    "        if random.random() < self.noise_prob:\n",
    "            distortions.append('noise')\n",
    "\n",
    "        # 2. Reverb\n",
    "        # Echo effect\n",
    "        # Delay the audio by 0.1 -> reduce volume -> pad it to original length -> add\n",
    "        if random.random() < self.reverb_prob:\n",
    "            distortions.append('reverb')\n",
    "\n",
    "        # 3. Shuffle\n",
    "        # Break into n segments and concat them randomly\n",
    "        if random.random() < self.shuffle_prob:\n",
    "            distortions.append('shuffle')\n",
    "\n",
    "        # 4. Time stretch\n",
    "        # Randomly slows (0.9) or speeds (1.1) the audio / doesn't change pitch\n",
    "        if random.random() < self.time_stretch_prob:\n",
    "            distortions.append('time_stretch')\n",
    "\n",
    "        # 5. Missing Gaps\n",
    "        # Randomly insert silences/gaps in the audio\n",
    "        if random.random() < self.gaps_prob:\n",
    "            distortions.append('missing_gaps')\n",
    "\n",
    "        # 6. Frequency Masking\n",
    "        # Randomly masks a range of frequencies in the spectrogram\n",
    "        #Butterworth filter is better than applying freqeuncy masks on spectogram (which already has frequency bins) because real wrld freq loss occurs during sound capture/transmission, affecting the raw audio.\n",
    "        #Butterworth simulates this situation by removing frequency content from the waveform which can then go thru the rest of pipeline,\n",
    "        #additionally it affects the phase relations and harmonics naturally in contrast to the crude zeroing of freq bins in spectogram\n",
    "\n",
    "        if random.random() < self.freq_mask_prob:\n",
    "            distortions.append('frequency_masking')\n",
    "\n",
    "\n",
    "\n",
    "        # Apply selected distortions\n",
    "        for distortion in distortions:\n",
    "            if distortion == 'noise':\n",
    "                audio = self._add_noise(audio)\n",
    "\n",
    "            elif distortion == 'reverb':\n",
    "                audio = self._add_reverb(audio)\n",
    "\n",
    "            elif distortion == 'shuffle':\n",
    "                audio = self._segment_shuffle(audio)\n",
    "\n",
    "            elif distortion == 'time_stretch':\n",
    "                audio = self._time_stretch(audio)\n",
    "\n",
    "            elif distortion == 'missing_gaps':\n",
    "                audio = self._add_missing_gaps(audio)\n",
    "\n",
    "            elif distortion == 'frequency_masking':\n",
    "                audio = self._add_frequency_mask(audio)\n",
    "\n",
    "        return audio\n",
    "    def _add_noise(self, audio):\n",
    "        return self.noise_aug(audio, self.sr)\n",
    "\n",
    "    def _add_reverb(self, audio):\n",
    "        delay = int(self.reverb_delay * self.sr)  # Delay in samples (0.05 sec)\n",
    "        reverb = tf.pad(audio * self.reverb_decay, [[delay, 0]]) #Amplitude scaling -> 0.2\n",
    "        reverb = reverb[:tf.shape(audio)[0]]\n",
    "        return audio + reverb\n",
    "\n",
    "    def _segment_shuffle(self, audio, n_segments=None):\n",
    "        # Previous method -> shuffle random large segments / unrealistic and destroys linguistic stuff\n",
    "        # if n_segments is None:\n",
    "        #     n_segments = self.shuffle_segments\n",
    "        # segments = np.array_split(audio, n_segments)\n",
    "        # np.random.shuffle(segments)\n",
    "        # return np.concatenate(segments)\n",
    "\n",
    "\n",
    "        #New method -> try to simulate temporal jitter / noise in the time domain -> split into micro-segments which are overlapping  -> the segments are shuffle locally within shuffle range\n",
    "        seg_len = int(self.shuffle_seg_dur * self.sr)\n",
    "        overlap = int(self.shuffle_overlap * self.sr)\n",
    "        local_range = self.shuffle_local_range\n",
    "        segments = []\n",
    "        i = 0\n",
    "        while i < len(audio):\n",
    "            end = min(i + seg_len, len(audio))\n",
    "            segments.append(audio[i:end])\n",
    "            i += seg_len - overlap\n",
    "        n_regions = min(4, max(1, len(segments) // 10))\n",
    "        region_indices = random.sample(range(len(segments)), n_regions)\n",
    "        shuffled = segments.copy()\n",
    "        for r in region_indices:\n",
    "            for offset in range(-local_range, local_range + 1):\n",
    "                idx = r + offset\n",
    "                if 0 <= idx < len(segments):\n",
    "                    shift = random.randint(-local_range, local_range)\n",
    "                    new_idx = max(0, min(len(segments) - 1, idx + shift))\n",
    "                    shuffled[idx] = segments[new_idx]\n",
    "        return np.concatenate(shuffled)[:len(audio)]\n",
    "\n",
    "\n",
    "    def _time_stretch(self, audio):\n",
    "        return tf.constant(librosa.effects.time_stretch(audio, rate=random.uniform(*self.time_stretch_range)),dtype = tf.float32)\n",
    "\n",
    "    def _add_missing_gaps(self, audio, n_gaps=None, max_gap_duration=None):\n",
    "        # if n_gaps is None:\n",
    "        #     n_gaps = self.gaps_n\n",
    "        # if max_gap_duration is None:\n",
    "        #     max_gap_duration = self.gaps_max_duration\n",
    "        # gap_audio = np.copy(audio)\n",
    "        # for _ in range(n_gaps):\n",
    "        #     gap_duration = random.uniform(0.1, max_gap_duration)\n",
    "        #     gap_samples = int(gap_duration * self.sr)\n",
    "        #     start = random.randint(0, max(1, len(audio) - gap_samples))\n",
    "        #     gap_audio[start:start + gap_samples] = 0\n",
    "        # return gap_audio\n",
    "\n",
    "        #New method -> fill gaps with low level noises and make edges smoother\n",
    "        if n_gaps is None:\n",
    "            n_gaps = self.gaps_n\n",
    "        if max_gap_duration is None:\n",
    "            max_gap_duration = self.gaps_max_duration\n",
    "\n",
    "        gap_audio = tf.identity(audio)\n",
    "\n",
    "        for _ in range(n_gaps):\n",
    "            gap_duration = tf.random.uniform([], 0.1, max_gap_duration, dtype=tf.float32)\n",
    "            gap_samples = tf.cast(gap_duration * tf.cast(self.sr, tf.float32), tf.int32)\n",
    "            start = tf.random.uniform([], 0, tf.shape(audio)[0] - gap_samples, dtype=tf.int32)\n",
    "\n",
    "            fade_len = tf.minimum(tf.cast(0.05 * tf.cast(gap_samples, tf.float32), tf.int32), gap_samples // 4)\n",
    "            fade_out = tf.cast(tf.linspace(1.0, 0.0, fade_len), tf.float32)\n",
    "            fade_in = tf.cast(tf.linspace(0.0, 1.0, fade_len), tf.float32)\n",
    "\n",
    "            mid_len = gap_samples - 2 * fade_len\n",
    "\n",
    "            if mid_len > 0:\n",
    "            #In case 0 values are needed.\n",
    "            #     if random.random() < 0.5:\n",
    "            #         gap_audio[start + fade_len:start + fade_len + mid_len] = 0\n",
    "            #     else:\n",
    "              noise = tf.random.normal([mid_len], stddev=0.001, dtype=tf.float32)\n",
    "              indices = tf.reshape(tf.range(start + fade_len, start + fade_len + mid_len), (-1, 1))\n",
    "              gap_audio = tf.tensor_scatter_nd_update(gap_audio, indices, noise)\n",
    "\n",
    "        fade_indices = tf.reshape(tf.range(start + fade_len + mid_len, start + gap_samples), (-1, 1))\n",
    "        fade_vals = gap_audio[start + fade_len + mid_len:start + gap_samples] * fade_in\n",
    "        gap_audio = tf.tensor_scatter_nd_update(gap_audio, fade_indices, fade_vals)\n",
    "\n",
    "        return gap_audio[:len(audio)]\n",
    "    #APPLY FREQUENCY MASKING USING BUTTERWORTH FILTER\n",
    "    #n_masks -> how many bands will be filtered out\n",
    "    #A 16 kHz sampler can only capture up to 8 kHz frequencies because you need at least two samples per wave cycle to know what the wave looks like -> nyquist\n",
    "    def _add_frequency_mask(self, audio, n_masks=None):\n",
    "      if n_masks is None:\n",
    "          n_masks = self.freq_mask_n\n",
    "      if isinstance(audio, tf.Tensor):\n",
    "        masked_audio = audio.numpy().copy()\n",
    "      else:\n",
    "        masked_audio = audio.copy()\n",
    "      nyquist = self.sr/2\n",
    "      for _ in range(n_masks):\n",
    "        l_freq = random.uniform(500,5000)\n",
    "        h_freq = l_freq + random.uniform(500,2000)\n",
    "        l_freq = min(l_freq,nyquist-100)\n",
    "        h_freq = min(h_freq,nyquist-100)\n",
    "        b,a = butter(N=4, Wn=[l_freq,h_freq], btype= \"bandstop\", fs = self.sr) #N=4 th order -> a dip in frequency response where that removal band is with smooth edges\n",
    "        masked_audio = lfilter(b,a,masked_audio)\n",
    "      return masked_audio\n",
    "augmenter=Augmenter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:45\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# 1. Set the path to your main dataset directory\n",
    "input_root_dir = \"/kaggle/input/cleaned-audio-milan/LibriSpeech/train-clean-100/\"\n",
    "\n",
    "# 2. Set the path where you want to save the new dataset\n",
    "output_root_dir = \"/kaggle/working/LibriSpeech-WAV-Complete/\"\n",
    "# ---\n",
    "\n",
    "augmenter=Augmenter()\n",
    "print(f\"Starting transfer and conversion from '{input_root_dir}'...\")\n",
    "print(f\"Output will be saved in '{output_root_dir}'.\")\n",
    "\n",
    "# Walk through the entire directory structure\n",
    "for dirpath, _, filenames in os.walk(input_root_dir):\n",
    "    for filename in filenames:\n",
    "        # Construct the full path to the source file\n",
    "        input_file_path = os.path.join(dirpath, filename)\n",
    "        \n",
    "        # Create the corresponding output directory structure\n",
    "        relative_path = os.path.relpath(dirpath, input_root_dir)\n",
    "        output_dir = os.path.join(output_root_dir, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # --- Logic to either convert or copy the file ---\n",
    "        try:\n",
    "            if filename.endswith(\".flac\"):\n",
    "                # It's an audio file, so convert it to WAV\n",
    "\n",
    "                # Create the full path for the output WAV file\n",
    "                wav_filename = Path(filename).stem + \".wav\"\n",
    "                output_file_path = os.path.join(output_dir, wav_filename)\n",
    "                audio, sr = librosa.load(input_file_path, sr=16000)\n",
    "                # Apply your custom augmentations\n",
    "                augmented_audio = augmenter.augment(audio)\n",
    "                sf.write(output_file_path, augmented_audio, sr)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                # It's a non-audio file (e.g., .txt), so copy it directly\n",
    "                \n",
    "                # Construct the output path\n",
    "                output_file_path = os.path.join(output_dir, filename)\n",
    "                \n",
    "                # Use shutil.copy2 to preserve file metadata\n",
    "                shutil.copy2(input_file_path, output_file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"New dataset is ready at: '{output_root_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.690225Z",
     "iopub.status.busy": "2025-10-19T19:06:28.689528Z",
     "iopub.status.idle": "2025-10-19T19:06:28.711751Z",
     "shell.execute_reply": "2025-10-19T19:06:28.710983Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.690200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "Training_dirs=\"/kaggle/working/LibriSpeech-WAV-Complete/\"\n",
    "# Parameters\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 # Set to a higher number for real training\n",
    "\n",
    "# Create a dummy vocabulary with only uppercase letters and apostrophe\n",
    "CHARACTERS = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    \"'\",' ']\n",
    "\n",
    "# Create character-to-number mappings\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\n",
    "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "VOCAB_SIZE = char_to_num.vocabulary_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.714011Z",
     "iopub.status.busy": "2025-10-19T19:06:28.713298Z",
     "iopub.status.idle": "2025-10-19T19:06:28.720483Z",
     "shell.execute_reply": "2025-10-19T19:06:28.719826Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.713980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to create dummy data\n",
    "def load_data():\n",
    "    file_paths = []\n",
    "    transcriptions = []\n",
    "    directories=[]\n",
    "    label_files=[]\n",
    "    for lfold1 in os.listdir(Training_dirs):\n",
    "        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n",
    "            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(full_path)\n",
    "                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n",
    "    for label_path in label_files:\n",
    "        with open(label_path,'r') as labels:\n",
    "            for line in labels.readlines():\n",
    "                transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n",
    "    for path in directories:\n",
    "        fp=[]\n",
    "        for file in os.listdir(path):\n",
    "            if(file.endswith('.wav')):\n",
    "                fp.append(os.path.join(path,file))\n",
    "        fp.sort()\n",
    "        file_paths+=fp\n",
    "    \n",
    "    print(len(label_files))\n",
    "        \n",
    "    return file_paths, transcriptions\n",
    "#load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.721444Z",
     "iopub.status.busy": "2025-10-19T19:06:28.721198Z",
     "iopub.status.idle": "2025-10-19T19:06:28.744372Z",
     "shell.execute_reply": "2025-10-19T19:06:28.743708Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.721426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. tf.data Pipeline (No Augmentation) ---\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n",
    "    try:\n",
    "        path_str = file_path.numpy().decode('utf-8')\n",
    "        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Compute the Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
    "        \n",
    "        # Convert to log scale (decibels)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n",
    "        \n",
    "        return log_mel_spec.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path.numpy()}: {e}\")\n",
    "        os.exit()\n",
    "        return np.zeros((100, 80, 1), dtype=np.float32)\n",
    "       \n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 80\n",
    "\n",
    "def power_to_db(S, ref=1.0, top_db=80.0):\n",
    "    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n",
    "    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n",
    "    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n",
    "    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "\n",
    "@tf.function\n",
    "def preprocess_audio_tf(file_path: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n",
    "    with padding to match librosa's default behavior.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "    \n",
    "        # decode_wav returns a normalized float32 tensor and the sample rate.\n",
    "        # desired_channels=1 ensures the audio is mono.\n",
    "        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n",
    "    \n",
    "        # Squeeze the channel dimension, leaving a 1D waveform.\n",
    "        # NO further normalization is needed.\n",
    "        waveform = tf.squeeze(audio_tensor, axis=-1)\n",
    "\n",
    "        # --- FIX: Manually pad the waveform to match librosa ---\n",
    "        # (The rest of your function remains the same and is correct)\n",
    "        padding = N_FFT // 2\n",
    "        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n",
    "        \n",
    "        # --- 2. Compute the STFT (The rest is the same) ---\n",
    "        stft = tf.signal.stft(\n",
    "            waveform,\n",
    "            frame_length=N_FFT,\n",
    "            frame_step=HOP_LENGTH,\n",
    "            fft_length=N_FFT\n",
    "        )\n",
    "        spectrogram = tf.abs(stft)\n",
    "\n",
    "        # ... (rest of the function is identical) ...\n",
    "        power_spectrogram = spectrogram ** 2\n",
    "        num_spectrogram_bins = stft.shape[-1]\n",
    "        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=N_MELS,\n",
    "            num_spectrogram_bins=num_spectrogram_bins,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            lower_edge_hertz=20.0,\n",
    "            upper_edge_hertz=8000.0\n",
    "        )\n",
    "        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n",
    "        log_mel_spectrogram = power_to_db(mel_spectrogram)\n",
    "        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n",
    "\n",
    "        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n",
    "        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n",
    "    \n",
    "def preprocess_label(text_label):\n",
    "    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n",
    "    # Convert all characters to uppercase to match the vocabulary\n",
    "    text_tensor = tf.strings.upper(text_label)\n",
    "    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n",
    "    return char_to_num(chars)\n",
    "# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n",
    "\n",
    "@tf.function\n",
    "def preprocess_and_filter(path, label):\n",
    "    \"\"\"\n",
    "    Applies full preprocessing to audio and text, and returns their lengths.\n",
    "    \"\"\"\n",
    "    # Process the audio file to get the final spectrogram\n",
    "    spectrogram = preprocess_audio_tf(path)\n",
    "    \n",
    "    # Process the text label to get the integer tokens\n",
    "    processed_label = preprocess_label(label)\n",
    "\n",
    "    # Get the number of time steps from the spectrogram\n",
    "    spectrogram_length = tf.shape(spectrogram)[0]\n",
    "    \n",
    "    # Get the number of characters/tokens from the label\n",
    "    label_length = tf.shape(processed_label)[0]\n",
    "\n",
    "    return spectrogram, processed_label, spectrogram_length, label_length\n",
    "#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.809637Z",
     "iopub.status.busy": "2025-10-19T19:06:28.808934Z",
     "iopub.status.idle": "2025-10-19T19:06:28.816166Z",
     "shell.execute_reply": "2025-10-19T19:06:28.815278Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.809613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # Map preprocessing functions\n",
    "    ds = ds.map(\n",
    "        lambda path, label: (\n",
    "            tf.py_function(preprocess_audio, [path], tf.float32),\n",
    "            preprocess_label(label)\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Batch and pad\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds'''\n",
    "\n",
    "def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # 1. Map the combined preprocessing and length calculation function\n",
    "    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 2. Filter out items where the spectrogram is shorter than the label\n",
    "    ds = ds.filter(\n",
    "        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n",
    "    )\n",
    "    \n",
    "    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n",
    "    ds = ds.map(\n",
    "        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # 4. Batch and pad as before\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:24:34.026671Z",
     "iopub.status.busy": "2025-10-19T19:24:34.026335Z",
     "iopub.status.idle": "2025-10-19T19:24:34.038987Z",
     "shell.execute_reply": "2025-10-19T19:24:34.038268Z",
     "shell.execute_reply.started": "2025-10-19T19:24:34.026647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, vocab_size):\n",
    "    \"\"\"Builds a deeper, more regularized CNN-RNN model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n",
    "\n",
    "    # Make the CNN frontend deeper\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "\n",
    "    # Reshape for the RNN\n",
    "    _, time_dim, freq_dim, channel_dim = x.shape\n",
    "    new_feature_dim = freq_dim * channel_dim\n",
    "    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n",
    "    \n",
    "    # Make the RNN backend deeper and with stronger dropout\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# --- 3. Model Definition and CTC Loss ---\n",
    "\n",
    "\"\"\"older model, has less layers but is proven to underfit given the data.\"\"\"\n",
    "def build_model_old1(input_shape, vocab_size):\n",
    "    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    _, time_dim, freq_dim, channel_dim = x.shape\n",
    "    new_feature_dim = freq_dim * channel_dim\n",
    "    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:06:28.835259Z",
     "iopub.status.busy": "2025-10-19T19:06:28.835041Z",
     "iopub.status.idle": "2025-10-19T19:06:28.851013Z",
     "shell.execute_reply": "2025-10-19T19:06:28.850375Z",
     "shell.execute_reply.started": "2025-10-19T19:06:28.835242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n",
    "    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    \n",
    "    # Compute actual label lengths\n",
    "    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n",
    "    label_length = tf.cast(label_length, dtype=\"int64\")\n",
    "    #label_length = tf.minimum(label_length, input_length)\n",
    "    \n",
    "    loss = tf.keras.backend.ctc_batch_cost(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        input_length,\n",
    "        label_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T19:24:40.639501Z",
     "iopub.status.busy": "2025-10-19T19:24:40.639197Z",
     "iopub.status.idle": "2025-10-19T21:44:49.866558Z",
     "shell.execute_reply": "2025-10-19T21:44:49.865857Z",
     "shell.execute_reply.started": "2025-10-19T19:24:40.639478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_spectrogram (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_54          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_55          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_56          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_57          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_25                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,147,776</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_58          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_26                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_59          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_27                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_60          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_spectrogram (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast_5 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_54          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_55          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_56          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_57          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_10 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_25                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,147,776\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_58          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_26                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_59          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_27                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_60          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │        \u001b[38;5;34m15,390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,384,894</span> (24.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,384,894\u001b[0m (24.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,381,438</span> (24.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,381,438\u001b[0m (24.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> (13.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,456\u001b[0m (13.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output time steps: 412\n",
      "Max label length in batch: tf.Tensor(282, shape=(), dtype=int64)\n",
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760901895.904535     561 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_10_1/spatial_dropout2d_2_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    803/Unknown \u001b[1m551s\u001b[0m 670ms/step - loss: 557.3900\n",
      "Epoch 1: val_loss improved from inf to 284.41974, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 707ms/step - loss: 557.2278 - val_loss: 284.4197\n",
      "Epoch 2/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - loss: 268.3391\n",
      "Epoch 2: val_loss improved from 284.41974 to 189.52693, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 701ms/step - loss: 268.3128 - val_loss: 189.5269\n",
      "Epoch 3/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - loss: 203.6213\n",
      "Epoch 3: val_loss improved from 189.52693 to 157.79767, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 700ms/step - loss: 203.6121 - val_loss: 157.7977\n",
      "Epoch 4/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - loss: 172.3438\n",
      "Epoch 4: val_loss improved from 157.79767 to 138.30461, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 696ms/step - loss: 172.3377 - val_loss: 138.3046\n",
      "Epoch 5/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - loss: 150.8970\n",
      "Epoch 5: val_loss improved from 138.30461 to 127.50397, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 695ms/step - loss: 150.8934 - val_loss: 127.5040\n",
      "Epoch 6/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - loss: 134.4951\n",
      "Epoch 6: val_loss improved from 127.50397 to 115.36959, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 695ms/step - loss: 134.4939 - val_loss: 115.3696\n",
      "Epoch 7/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659ms/step - loss: 123.6845\n",
      "Epoch 7: val_loss improved from 115.36959 to 110.11495, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 693ms/step - loss: 123.6835 - val_loss: 110.1150\n",
      "Epoch 8/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659ms/step - loss: 115.2150\n",
      "Epoch 8: val_loss improved from 110.11495 to 105.13895, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 693ms/step - loss: 115.2137 - val_loss: 105.1389\n",
      "Epoch 9/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - loss: 108.4321\n",
      "Epoch 9: val_loss improved from 105.13895 to 100.48463, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 697ms/step - loss: 108.4314 - val_loss: 100.4846\n",
      "Epoch 10/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - loss: 103.5762\n",
      "Epoch 10: val_loss improved from 100.48463 to 96.89747, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 699ms/step - loss: 103.5751 - val_loss: 96.8975\n",
      "Epoch 11/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - loss: 96.4322\n",
      "Epoch 11: val_loss improved from 96.89747 to 93.37787, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 695ms/step - loss: 96.4332 - val_loss: 93.3779\n",
      "Epoch 12/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - loss: 93.3550\n",
      "Epoch 12: val_loss improved from 93.37787 to 90.65246, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 696ms/step - loss: 93.3553 - val_loss: 90.6525\n",
      "Epoch 13/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - loss: 90.4108\n",
      "Epoch 13: val_loss improved from 90.65246 to 88.72932, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 694ms/step - loss: 90.4107 - val_loss: 88.7293\n",
      "Epoch 14/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - loss: 87.9924\n",
      "Epoch 14: val_loss did not improve from 88.72932\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 696ms/step - loss: 87.9935 - val_loss: 89.7892\n",
      "Epoch 15/15\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658ms/step - loss: 86.1120\n",
      "Epoch 15: val_loss improved from 88.72932 to 87.93477, saving model to asr_model_best.keras\n",
      "\u001b[1m803/803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 691ms/step - loss: 86.1123 - val_loss: 87.9348\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Training and Saving Logic ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate the dataset\n",
    "    paths, labels = load_data()\n",
    "    \n",
    "    # Split data (simple split for demonstration)\n",
    "    split_idx = int(len(paths) * 0.9)\n",
    "    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n",
    "    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # Build data pipelines\n",
    "    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n",
    "    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n",
    "    \n",
    "    steps_per_epoch = len(train_paths) // BATCH_SIZE\n",
    "    total_decay_steps = steps_per_epoch * EPOCHS\n",
    "    \n",
    "    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=1e-2,  # The starting learning rate\n",
    "        decay_steps=total_decay_steps, # The number of steps to decay over\n",
    "        alpha=0 # The minimum learning rate as a fraction of the initial rate\n",
    "    )\n",
    "    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n",
    "    # Build the model\n",
    "    # We don't know the exact input shape, so we use None for the time dimension\n",
    "    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n",
    "    model.compile(optimizer=\"adam\", loss=ctc_loss)\n",
    "    \n",
    "    model.summary()\n",
    "    for x_batch, y_batch in train_ds.take(1):\n",
    "        preds = model(x_batch)\n",
    "        print(\"Model output time steps:\", preds.shape[1])\n",
    "        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n",
    "\n",
    "    # Set up callbacks\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"asr_model_best.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1\n",
    "    )\n",
    "    # Train the model\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[model_checkpoint]\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T21:49:32.805784Z",
     "iopub.status.busy": "2025-10-19T21:49:32.805509Z",
     "iopub.status.idle": "2025-10-19T21:49:33.453078Z",
     "shell.execute_reply": "2025-10-19T21:49:33.452272Z",
     "shell.execute_reply.started": "2025-10-19T21:49:32.805765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training complete. Final model saved as asr_model_final.keras ---\n",
      "Best performing model during training saved as asr_model_best.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Save the final model\n",
    "    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n",
    "    print(\"\\n--- Training complete. Final model saved as asr_model_final.keras ---\")\n",
    "    print(\"Best performing model during training saved as asr_model_best.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-19T19:11:18.909098Z",
     "iopub.status.idle": "2025-10-19T19:11:18.909417Z",
     "shell.execute_reply": "2025-10-19T19:11:18.909262Z",
     "shell.execute_reply.started": "2025-10-19T19:11:18.909247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#quantised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\n",
    "model = tf.keras.models.load_model(\"/kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n",
    "\n",
    "# (Optionally) Lower LR manually before continuing\n",
    "tf.keras.backend.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "\n",
    "# Recreate the same callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"asr_model_best.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Resume training from epoch 15 → 25\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    initial_epoch=15,\n",
    "    callbacks=[model_checkpoint, reduce_lr]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
