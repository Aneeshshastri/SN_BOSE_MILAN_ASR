{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:47.415857Z",
     "iopub.status.busy": "2025-10-22T04:37:47.415504Z",
     "iopub.status.idle": "2025-10-22T04:37:51.964052Z",
     "shell.execute_reply": "2025-10-22T04:37:51.962646Z",
     "shell.execute_reply.started": "2025-10-22T04:37:47.415835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:51.966778Z",
     "iopub.status.busy": "2025-10-22T04:37:51.966382Z",
     "iopub.status.idle": "2025-10-22T04:37:51.974198Z",
     "shell.execute_reply": "2025-10-22T04:37:51.972511Z",
     "shell.execute_reply.started": "2025-10-22T04:37:51.966750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "from scipy.signal import butter, lfilter\n",
    "from IPython.display import Audio\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:51.975926Z",
     "iopub.status.busy": "2025-10-22T04:37:51.975622Z",
     "iopub.status.idle": "2025-10-22T04:37:52.011038Z",
     "shell.execute_reply": "2025-10-22T04:37:52.009602Z",
     "shell.execute_reply.started": "2025-10-22T04:37:51.975903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "#set it to the directory where your input folder is\n",
    "input_root_dir=\"\"\n",
    "output_root_dir = \"\"\n",
    "# ---\n",
    "Training_dirs=output_root_dir\n",
    "# Parameters\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 #For half trained model \n",
    "# Create a vocabulary with only uppercase letters and apostrophe\n",
    "CHARACTERS = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    \"'\",' ']\n",
    "\n",
    "# Create character-to-number mappings\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\n",
    "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "VOCAB_SIZE = char_to_num.vocabulary_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2.Converts the noise-augmented .flac files to .wav\n",
    "print(f\"Starting transfer and conversion from '{input_root_dir}'...\")\n",
    "print(f\"Output will be saved in '{output_root_dir}'.\")\n",
    "\n",
    "# Walk through the entire directory structure\n",
    "for dirpath, _, filenames in os.walk(input_root_dir):\n",
    "    for filename in filenames:\n",
    "        # Construct the full path to the source file\n",
    "        input_file_path = os.path.join(dirpath, filename)\n",
    "        \n",
    "        # Determine the corresponding output directory path\n",
    "        relative_path = os.path.relpath(dirpath, input_root_dir)\n",
    "        output_dir = os.path.join(output_root_dir, relative_path)\n",
    "        \n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # --- Logic to either convert or copy the file ---\n",
    "        try:\n",
    "            if filename.lower().endswith(\".flac\"):\n",
    "                # It's an audio file, so convert it to WAV\n",
    "                \n",
    "                # Create the full path for the output WAV file\n",
    "                wav_filename = Path(filename).stem + \".wav\"\n",
    "                output_file_path = os.path.join(output_dir, wav_filename)\n",
    "\n",
    "                # Read the FLAC data and write it as WAV\n",
    "                # Using soundfile which handles both reading FLAC and writing WAV\n",
    "                audio_data, sample_rate = sf.read(input_file_path)\n",
    "                sf.write(output_file_path, audio_data, sample_rate)\n",
    "                # Optional: Print progress for audio files\n",
    "                # print(f\"Converted: {input_file_path} -> {output_file_path}\")\n",
    "\n",
    "            else:\n",
    "                # It's a non-audio file (e.g., .txt), so copy it directly\n",
    "                \n",
    "                # Construct the output path for the copied file\n",
    "                output_file_path = os.path.join(output_dir, filename)\n",
    "                shutil.copy2(input_file_path, output_file_path)\n",
    "                # Optional: Print progress for copied files\n",
    "                # print(f\"Copied: {input_file_path} -> {output_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print an error message if any file fails to process\n",
    "            print(f\"Error processing {input_file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"New dataset with WAV files is ready at: '{output_root_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.014886Z",
     "iopub.status.busy": "2025-10-22T04:37:52.013763Z",
     "iopub.status.idle": "2025-10-22T04:37:52.028563Z",
     "shell.execute_reply": "2025-10-22T04:37:52.027254Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.014850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to load my new .wav dataset\n",
    "def load_data():\n",
    "    file_paths = []\n",
    "    transcriptions = []\n",
    "    directories=[]\n",
    "    label_files=[]\n",
    "    for lfold1 in os.listdir(Training_dirs):\n",
    "        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n",
    "            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(full_path)\n",
    "                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n",
    "        for label_path in label_files:\n",
    "            with open(label_path,'r') as labels:\n",
    "                for line in labels.readlines():\n",
    "                    transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n",
    "        for path in directories:\n",
    "            fp=[]\n",
    "            for file in os.listdir(path):\n",
    "                if(file.endswith('.wav')):\n",
    "                    fp.append(os.path.join(path,file))\n",
    "            fp.sort()\n",
    "            file_paths+=fp\n",
    "    \n",
    "    print(len(label_files))\n",
    "        \n",
    "    return file_paths, transcriptionsi\n",
    "#print(os.listdir(Training_dirs))\n",
    "#load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.030969Z",
     "iopub.status.busy": "2025-10-22T04:37:52.030589Z",
     "iopub.status.idle": "2025-10-22T04:37:52.050818Z",
     "shell.execute_reply": "2025-10-22T04:37:52.049580Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.030934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. tf.data Pipeline--\n",
    "#preprocess using librosa (works on .flac)\n",
    "'''def preprocess_audio(file_path):\n",
    "    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n",
    "    try:\n",
    "        path_str = file_path.numpy().decode('utf-8')\n",
    "        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Compute the Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
    "        \n",
    "        # Convert to log scale (decibels)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n",
    "        \n",
    "        return log_mel_spec.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path.numpy()}: {e}\")\n",
    "        os.exit()\n",
    "        return np.zeros((100, 80, 1), dtype=np.float32)'''\n",
    "       \n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 80\n",
    "\n",
    "def power_to_db(S, ref=1.0, top_db=80.0):\n",
    "    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n",
    "    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n",
    "    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n",
    "    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "#using tensorflow: requires .wav but significantly faster\n",
    "@tf.function\n",
    "def preprocess_audio_tf(file_path: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n",
    "    with padding to match librosa's default behavior.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "    \n",
    "        # decode_wav returns a normalized float32 tensor and the sample rate.\n",
    "        # desired_channels=1 ensures the audio is mono.\n",
    "        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n",
    "    \n",
    "        # Squeeze the channel dimension, leaving a 1D waveform.\n",
    "        # NO further normalization is needed.\n",
    "        waveform = tf.squeeze(audio_tensor, axis=-1)\n",
    "\n",
    "        # --- FIX: Manually pad the waveform to match librosa ---\n",
    "        # (The rest of your function remains the same and is correct)\n",
    "        padding = N_FFT // 2\n",
    "        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n",
    "        \n",
    "        # --- 2. Compute the STFT (The rest is the same) ---\n",
    "        stft = tf.signal.stft(\n",
    "            waveform,\n",
    "            frame_length=N_FFT,\n",
    "            frame_step=HOP_LENGTH,\n",
    "            fft_length=N_FFT\n",
    "        )\n",
    "        spectrogram = tf.abs(stft)\n",
    "\n",
    "        # ... (rest of the function is identical) ...\n",
    "        power_spectrogram = spectrogram ** 2\n",
    "        num_spectrogram_bins = stft.shape[-1]\n",
    "        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=N_MELS,\n",
    "            num_spectrogram_bins=num_spectrogram_bins,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            lower_edge_hertz=20.0,\n",
    "            upper_edge_hertz=8000.0\n",
    "        )\n",
    "        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n",
    "        log_mel_spectrogram = power_to_db(mel_spectrogram)\n",
    "        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n",
    "\n",
    "        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n",
    "        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n",
    "    \n",
    "def preprocess_label(text_label):\n",
    "    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n",
    "    # Convert all characters to uppercase to match the vocabulary\n",
    "    text_tensor = tf.strings.upper(text_label)\n",
    "    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n",
    "    return char_to_num(chars)\n",
    "# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n",
    "\n",
    "@tf.function\n",
    "def preprocess_and_filter(path, label):\n",
    "    \"\"\"\n",
    "    Applies full preprocessing to audio and text, and returns their lengths.\n",
    "    \"\"\"\n",
    "    # Process the audio file to get the final spectrogram\n",
    "    spectrogram = preprocess_audio_tf(path)\n",
    "    \n",
    "    # Process the text label to get the integer tokens\n",
    "    processed_label = preprocess_label(label)\n",
    "\n",
    "    # Get the number of time steps from the spectrogram\n",
    "    spectrogram_length = tf.shape(spectrogram)[0]\n",
    "    \n",
    "    # Get the number of characters/tokens from the label\n",
    "    label_length = tf.shape(processed_label)[0]\n",
    "\n",
    "    return spectrogram, processed_label, spectrogram_length, label_length\n",
    "#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.052121Z",
     "iopub.status.busy": "2025-10-22T04:37:52.051856Z",
     "iopub.status.idle": "2025-10-22T04:37:52.080823Z",
     "shell.execute_reply": "2025-10-22T04:37:52.079644Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.052098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # Map preprocessing functions\n",
    "    ds = ds.map(\n",
    "        lambda path, label: (\n",
    "            tf.py_function(preprocess_audio, [path], tf.float32),\n",
    "            preprocess_label(label)\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Batch and pad\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "    ||OLDER PIPELINE THAT USES LIBROSA||\n",
    "    '''\n",
    "\n",
    "def build_pipeline(paths, labels, is_training=False):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    \n",
    "    # 1. Map the combined preprocessing and length calculation function\n",
    "    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 2. Filter out items where the spectrogram is shorter than the label\n",
    "    ds = ds.filter(\n",
    "        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n",
    "    )\n",
    "    \n",
    "    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n",
    "    ds = ds.map(\n",
    "        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # 4. Batch and pad as before\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padded_shapes=([None, 80, 1], [None]),\n",
    "        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n",
    "    )\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.082352Z",
     "iopub.status.busy": "2025-10-22T04:37:52.081965Z",
     "iopub.status.idle": "2025-10-22T04:37:52.113000Z",
     "shell.execute_reply": "2025-10-22T04:37:52.111323Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.082321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, vocab_size):\n",
    "    \"\"\"Builds a layered CNN-RNN model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n",
    "\n",
    "    # Make the CNN frontend deeper\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n",
    "\n",
    "    # Reshape for the RNN\n",
    "    _, time_dim, freq_dim, channel_dim = x.shape\n",
    "    new_feature_dim = freq_dim * channel_dim\n",
    "    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n",
    "    \n",
    "    # Make the RNN backend deeper and with stronger dropout\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.114710Z",
     "iopub.status.busy": "2025-10-22T04:37:52.114323Z",
     "iopub.status.idle": "2025-10-22T04:37:52.137558Z",
     "shell.execute_reply": "2025-10-22T04:37:52.136412Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.114672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#CUSTOM LOSS FUNCTION REQUIRED TO TRAIN MY MODEL.\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n",
    "    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    \n",
    "    # Compute actual label lengths\n",
    "    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n",
    "    label_length = tf.cast(label_length, dtype=\"int64\")\n",
    "    #label_length = tf.minimum(label_length, input_length)\n",
    "    \n",
    "    loss = tf.keras.backend.ctc_batch_cost(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        input_length,\n",
    "        label_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T04:37:52.139271Z",
     "iopub.status.busy": "2025-10-22T04:37:52.138809Z",
     "iopub.status.idle": "2025-10-22T04:37:52.431339Z",
     "shell.execute_reply": "2025-10-22T04:37:52.429516Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.139239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. Main Training and Saving Logic ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate the dataset\n",
    "    paths, labels = load_data()\n",
    "    \n",
    "    # Split data (simple split for demonstration)\n",
    "    split_idx = int(len(paths) * 0.9)\n",
    "    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n",
    "    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # Build data pipelines\n",
    "    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n",
    "    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n",
    "    \n",
    "    steps_per_epoch = len(train_paths) // BATCH_SIZE\n",
    "    total_decay_steps = steps_per_epoch * EPOCHS\n",
    "    \n",
    "    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=1e-2,  # The starting learning rate\n",
    "        decay_steps=total_decay_steps, # The number of steps to decay over\n",
    "        alpha=0 # The minimum learning rate as a fraction of the initial rate\n",
    "    )\n",
    "    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n",
    "    # Build the model\n",
    "    # We don't know the exact input shape, so we use None for the time dimension\n",
    "    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n",
    "    model.compile(optimizer=Optimizer, loss=ctc_loss)\n",
    "    \n",
    "    model.summary()\n",
    "    for x_batch, y_batch in train_ds.take(1):\n",
    "        preds = model(x_batch)\n",
    "        print(\"Model output time steps:\", preds.shape[1])\n",
    "        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n",
    "\n",
    "    # Set up callbacks\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"asr_model_best.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1\n",
    "    )\n",
    "    # Train the model\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[model_checkpoint]\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T04:37:52.432026Z",
     "iopub.status.idle": "2025-10-22T04:37:52.432394Z",
     "shell.execute_reply": "2025-10-22T04:37:52.432238Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.432218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Save the half-trained model\n",
    "    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n",
    "    print(\"\\n---Training  Part 1 complete. Final model saved ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T04:37:52.434348Z",
     "iopub.status.idle": "2025-10-22T04:37:52.434826Z",
     "shell.execute_reply": "2025-10-22T04:37:52.434635Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.434614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T04:37:52.438935Z",
     "iopub.status.idle": "2025-10-22T04:37:52.440091Z",
     "shell.execute_reply": "2025-10-22T04:37:52.439773Z",
     "shell.execute_reply.started": "2025-10-22T04:37:52.439476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\n",
    "model = tf.keras.models.load_model(\"/kaggle/working/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n",
    "\n",
    "#Lower LR manually before continuing\n",
    "tf.keras.backend.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "\n",
    "# Recreate the same callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"asr_model_best.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Resume training from epoch 15 â†’ 25\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    initial_epoch=15,\n",
    "    callbacks=[model_checkpoint, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE FINAL MODEL\n",
    "model.save(\"/kaggle/working/asr_model_final_ep25.keras\")\n",
    "print(\"\\n---Training  Part 2 complete. Final model saved ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8544558,
     "sourceId": 13461226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
