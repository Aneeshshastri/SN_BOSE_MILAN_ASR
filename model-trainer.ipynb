{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13461226,"sourceType":"datasetVersion","datasetId":8544558}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:47.415504Z","iopub.execute_input":"2025-10-22T04:37:47.415857Z","iopub.status.idle":"2025-10-22T04:37:51.964052Z","shell.execute_reply.started":"2025-10-22T04:37:47.415835Z","shell.execute_reply":"2025-10-22T04:37:51.962646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport os\nimport matplotlib.pyplot as plt\nimport shutil\nfrom pathlib import Path\nimport random\nfrom scipy.signal import butter, lfilter\nfrom IPython.display import Audio\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:51.966382Z","iopub.execute_input":"2025-10-22T04:37:51.966778Z","iopub.status.idle":"2025-10-22T04:37:51.974198Z","shell.execute_reply.started":"2025-10-22T04:37:51.966750Z","shell.execute_reply":"2025-10-22T04:37:51.972511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- 1. Configuration ---\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ninput_root_dir=\"\"\noutput_root_dir = \"\"\n# ---\nTraining_dirs=output_root_dir\n# Parameters\nSAMPLE_RATE = 16000\nBATCH_SIZE = 32\nEPOCHS = 15 # Set to a higher number for real training\n\n# Create a dummy vocabulary with only uppercase letters and apostrophe\nCHARACTERS = [\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    \"'\",' ']\n\n# Create character-to-number mappings\nchar_to_num = tf.keras.layers.StringLookup(vocabulary=list(CHARACTERS), mask_token=None)\nnum_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\nVOCAB_SIZE = char_to_num.vocabulary_size()\n","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:51.975622Z","iopub.execute_input":"2025-10-22T04:37:51.975926Z","iopub.status.idle":"2025-10-22T04:37:52.011038Z","shell.execute_reply.started":"2025-10-22T04:37:51.975903Z","shell.execute_reply":"2025-10-22T04:37:52.009602Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Set the path where you want to save the new dataset (with WAV files)\n\n\nprint(f\"Starting transfer and conversion from '{input_root_dir}'...\")\nprint(f\"Output will be saved in '{output_root_dir}'.\")\n\n# Walk through the entire directory structure\nfor dirpath, _, filenames in os.walk(input_root_dir):\n    for filename in filenames:\n        # Construct the full path to the source file\n        input_file_path = os.path.join(dirpath, filename)\n        \n        # Determine the corresponding output directory path\n        relative_path = os.path.relpath(dirpath, input_root_dir)\n        output_dir = os.path.join(output_root_dir, relative_path)\n        \n        # Create the output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        # --- Logic to either convert or copy the file ---\n        try:\n            if filename.lower().endswith(\".flac\"):\n                # It's an audio file, so convert it to WAV\n                \n                # Create the full path for the output WAV file\n                wav_filename = Path(filename).stem + \".wav\"\n                output_file_path = os.path.join(output_dir, wav_filename)\n\n                # Read the FLAC data and write it as WAV\n                # Using soundfile which handles both reading FLAC and writing WAV\n                audio_data, sample_rate = sf.read(input_file_path)\n                sf.write(output_file_path, audio_data, sample_rate)\n                # Optional: Print progress for audio files\n                # print(f\"Converted: {input_file_path} -> {output_file_path}\")\n\n            else:\n                # It's a non-audio file (e.g., .txt), so copy it directly\n                \n                # Construct the output path for the copied file\n                output_file_path = os.path.join(output_dir, filename)\n                \n                # Use shutil.copy2 to preserve file metadata (like modification time)\n                shutil.copy2(input_file_path, output_file_path)\n                # Optional: Print progress for copied files\n                # print(f\"Copied: {input_file_path} -> {output_file_path}\")\n\n        except Exception as e:\n            # Print an error message if any file fails to process\n            print(f\"Error processing {input_file_path}: {e}\")\n\nprint(\"\\n--- Process Complete ---\")\nprint(f\"New dataset with WAV files is ready at: '{output_root_dir}'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to create dummy data\ndef load_data():\n    file_paths = []\n    transcriptions = []\n    directories=[]\n    label_files=[]\n    for lfold1 in os.listdir(Training_dirs):\n       if(lfold):\n        for lfold2 in os.listdir(os.path.join(Training_dirs,lfold1)):\n            full_path = os.path.join(Training_dirs, lfold1,lfold2)\n            if os.path.isdir(full_path):\n                directories.append(full_path)\n                label_files.append(os.path.join(Training_dirs, lfold1,lfold2,lfold1+'-'+lfold2+'.trans.txt'))\n        for label_path in label_files:\n            with open(label_path,'r') as labels:\n                for line in labels.readlines():\n                    transcriptions.append(line.split(' ',maxsplit=1)[1].strip())\n        for path in directories:\n            fp=[]\n            for file in os.listdir(path):\n                if(file.endswith('.wav')):\n                    fp.append(os.path.join(path,file))\n            fp.sort()\n            file_paths+=fp\n    \n    print(len(label_files))\n        \n    return file_paths, transcriptions\n#print(os.listdir(Training_dirs))\n#load_data()","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.013763Z","iopub.execute_input":"2025-10-22T04:37:52.014886Z","iopub.status.idle":"2025-10-22T04:37:52.028563Z","shell.execute_reply.started":"2025-10-22T04:37:52.014850Z","shell.execute_reply":"2025-10-22T04:37:52.027254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. tf.data Pipeline (No Augmentation) ---\n\ndef preprocess_audio(file_path):\n    \"\"\"Loads and converts a FLAC file to a log Mel spectrogram.\"\"\"\n    try:\n        path_str = file_path.numpy().decode('utf-8')\n        y, sr = librosa.load(path_str, sr=SAMPLE_RATE)\n        \n        # Compute the Mel spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n        \n        # Convert to log scale (decibels)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Add a channel dimension\n        log_mel_spec = np.expand_dims(log_mel_spec.T, axis=-1)\n        \n        return log_mel_spec.astype(np.float32)\n    except Exception as e:\n        print(f\"Error processing file {file_path.numpy()}: {e}\")\n        os.exit()\n        return np.zeros((100, 80, 1), dtype=np.float32)\n       \n\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nN_MELS = 80\n\ndef power_to_db(S, ref=1.0, top_db=80.0):\n    \"\"\"Converts a power spectrogram to the decibel scale.\"\"\"\n    log_spec = 10.0 * (tf.math.log(tf.maximum(S, 1e-10)) / tf.math.log(10.0))\n    log_spec -= 10.0 * (tf.math.log(tf.maximum(ref, 1e-10)) / tf.math.log(10.0))\n    return tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n\n@tf.function\ndef preprocess_audio_tf(file_path: tf.Tensor):\n    \"\"\"\n    Loads and converts a FLAC file to a log Mel spectrogram using TensorFlow,\n    with padding to match librosa's default behavior.\n    \"\"\"\n    try:\n        \n        audio_binary = tf.io.read_file(file_path)\n    \n        # decode_wav returns a normalized float32 tensor and the sample rate.\n        # desired_channels=1 ensures the audio is mono.\n        audio_tensor, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)\n    \n        # Squeeze the channel dimension, leaving a 1D waveform.\n        # NO further normalization is needed.\n        waveform = tf.squeeze(audio_tensor, axis=-1)\n\n        # --- FIX: Manually pad the waveform to match librosa ---\n        # (The rest of your function remains the same and is correct)\n        padding = N_FFT // 2\n        waveform = tf.pad(waveform, [[padding, padding]], mode=\"REFLECT\")\n        \n        # --- 2. Compute the STFT (The rest is the same) ---\n        stft = tf.signal.stft(\n            waveform,\n            frame_length=N_FFT,\n            frame_step=HOP_LENGTH,\n            fft_length=N_FFT\n        )\n        spectrogram = tf.abs(stft)\n\n        # ... (rest of the function is identical) ...\n        power_spectrogram = spectrogram ** 2\n        num_spectrogram_bins = stft.shape[-1]\n        mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n            num_mel_bins=N_MELS,\n            num_spectrogram_bins=num_spectrogram_bins,\n            sample_rate=SAMPLE_RATE,\n            lower_edge_hertz=20.0,\n            upper_edge_hertz=8000.0\n        )\n        mel_spectrogram = tf.tensordot(power_spectrogram, mel_filterbank, 1)\n        log_mel_spectrogram = power_to_db(mel_spectrogram)\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, axis=-1)\n\n        return tf.cast(log_mel_spectrogram, dtype=tf.float32)\n\n    except Exception as e:\n        tf.print(\"Error processing file:\", file_path, \"Exception:\", e, summarize=-1)\n        return tf.zeros((100, N_MELS, 1), dtype=tf.float32)\n    \ndef preprocess_label(text_label):\n    \"\"\"Converts a text string to an integer sequence, ensuring it's uppercase.\"\"\"\n    # Convert all characters to uppercase to match the vocabulary\n    text_tensor = tf.strings.upper(text_label)\n    chars = tf.strings.unicode_split(text_tensor, input_encoding=\"UTF-8\")\n    return char_to_num(chars)\n# (Keep all your other functions like preprocess_audio_tf_flac, preprocess_label, etc.)\n\n@tf.function\ndef preprocess_and_filter(path, label):\n    \"\"\"\n    Applies full preprocessing to audio and text, and returns their lengths.\n    \"\"\"\n    # Process the audio file to get the final spectrogram\n    spectrogram = preprocess_audio_tf(path)\n    \n    # Process the text label to get the integer tokens\n    processed_label = preprocess_label(label)\n\n    # Get the number of time steps from the spectrogram\n    spectrogram_length = tf.shape(spectrogram)[0]\n    \n    # Get the number of characters/tokens from the label\n    label_length = tf.shape(processed_label)[0]\n\n    return spectrogram, processed_label, spectrogram_length, label_length\n#preprocess_audio_tf(\"/kaggle/working/LibriSpeech-WAV-Complete/1081/125237/1081-125237-0035.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.030589Z","iopub.execute_input":"2025-10-22T04:37:52.030969Z","iopub.status.idle":"2025-10-22T04:37:52.050818Z","shell.execute_reply.started":"2025-10-22T04:37:52.030934Z","shell.execute_reply":"2025-10-22T04:37:52.049580Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''def build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=len(paths))\n    \n    # Map preprocessing functions\n    ds = ds.map(\n        lambda path, label: (\n            tf.py_function(preprocess_audio, [path], tf.float32),\n            preprocess_label(label)\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # Batch and pad\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(char_to_num.vocabulary_size(), dtype=tf.int64)+1)\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds'''\n\ndef build_pipeline(paths, labels, is_training=False):\n    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((path_ds, label_ds))\n    if is_training:\n        ds = ds.shuffle(buffer_size=len(paths))\n    \n    # 1. Map the combined preprocessing and length calculation function\n    ds = ds.map(preprocess_and_filter, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # 2. Filter out items where the spectrogram is shorter than the label\n    ds = ds.filter(\n        lambda spectrogram, label, spec_len, label_len: spec_len >= label_len\n    )\n    \n    # 3. Remove the lengths from the dataset, keeping only spectrogram and label\n    ds = ds.map(\n        lambda spectrogram, label, spec_len, label_len: (spectrogram, label),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # 4. Batch and pad as before\n    ds = ds.padded_batch(\n        batch_size=BATCH_SIZE,\n        padded_shapes=([None, 80, 1], [None]),\n        padding_values=(0.0, tf.cast(0, dtype=tf.int64))\n    )\n    \n    # Prefetch for performance\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.051856Z","iopub.execute_input":"2025-10-22T04:37:52.052121Z","iopub.status.idle":"2025-10-22T04:37:52.080823Z","shell.execute_reply.started":"2025-10-22T04:37:52.052098Z","shell.execute_reply":"2025-10-22T04:37:52.079644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(input_shape, vocab_size):\n    \"\"\"Builds a deeper, more regularized CNN-RNN model.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    # Make the CNN frontend deeper\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x) # <-- Add SpatialDropout\n\n    # Reshape for the RNN\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n    \n    # Make the RNN backend deeper and with stronger dropout\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Dropout(0.4)(x) # <-- Increased Dropout\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    # Output layer\n    outputs = tf.keras.layers.Dense(units=vocab_size + 1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# --- 3. Model Definition and CTC Loss ---\n\n\"\"\"older model, has less layers but is proven to underfit given the data.\"\"\"\ndef build_model_old1(input_shape, vocab_size):\n    \"\"\"Builds a CNN-RNN model with CTC loss.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape, name=\"input_spectrogram\")\n\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    \n    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    _, time_dim, freq_dim, channel_dim = x.shape\n    new_feature_dim = freq_dim * channel_dim\n    x = tf.keras.layers.Reshape((time_dim, new_feature_dim))(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    outputs = tf.keras.layers.Dense(units=vocab_size+1, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.081965Z","iopub.execute_input":"2025-10-22T04:37:52.082352Z","iopub.status.idle":"2025-10-22T04:37:52.113000Z","shell.execute_reply.started":"2025-10-22T04:37:52.082321Z","shell.execute_reply":"2025-10-22T04:37:52.111323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef ctc_loss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_pred)[0], dtype=\"int64\")\n    time_steps = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n\n    input_length = time_steps * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    \n    # Compute actual label lengths\n    label_length = tf.math.count_nonzero(y_true, axis=1, keepdims=True)\n    label_length = tf.cast(label_length, dtype=\"int64\")\n    #label_length = tf.minimum(label_length, input_length)\n    \n    loss = tf.keras.backend.ctc_batch_cost(\n        y_true,\n        y_pred,\n        input_length,\n        label_length,\n    )\n\n\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.114323Z","iopub.execute_input":"2025-10-22T04:37:52.114710Z","iopub.status.idle":"2025-10-22T04:37:52.137558Z","shell.execute_reply.started":"2025-10-22T04:37:52.114672Z","shell.execute_reply":"2025-10-22T04:37:52.136412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Main Training and Saving Logic ---\n\nif __name__ == \"__main__\":\n    # Generate the dataset\n    paths, labels = load_data()\n    \n    # Split data (simple split for demonstration)\n    split_idx = int(len(paths) * 0.9)\n    train_paths, val_paths = paths[:split_idx], paths[split_idx:]\n    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n    \n    # Build data pipelines\n    train_ds = build_pipeline(train_paths, train_labels, is_training=True)\n    val_ds = build_pipeline(val_paths, val_labels, is_training=False)\n    \n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    total_decay_steps = steps_per_epoch * EPOCHS\n    \n    cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-2,  # The starting learning rate\n        decay_steps=total_decay_steps, # The number of steps to decay over\n        alpha=0 # The minimum learning rate as a fraction of the initial rate\n    )\n    Optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_schedule)\n    # Build the model\n    # We don't know the exact input shape, so we use None for the time dimension\n    model = build_model(input_shape=(None, 80, 1), vocab_size=VOCAB_SIZE)\n    model.compile(optimizer=Optimizer, loss=ctc_loss)\n    \n    model.summary()\n    for x_batch, y_batch in train_ds.take(1):\n        preds = model(x_batch)\n        print(\"Model output time steps:\", preds.shape[1])\n        print(\"Max label length in batch:\", tf.reduce_max(tf.math.count_nonzero(y_batch, axis=1)))\n\n    # Set up callbacks\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"asr_model_best.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n        verbose=1\n    )\n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=[model_checkpoint]\n    )\n   ","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.138809Z","iopub.execute_input":"2025-10-22T04:37:52.139271Z","iopub.status.idle":"2025-10-22T04:37:52.431339Z","shell.execute_reply.started":"2025-10-22T04:37:52.139239Z","shell.execute_reply":"2025-10-22T04:37:52.429516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    # Save the final model\n    model.save(\"/kaggle/working/asr_model_final_ep15.keras\")\n    print(\"\\n--- Training complete. Final model saved as asr_model_final.keras ---\")\n    print(\"Best performing model during training saved as asr_model_best.keras\")","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.432026Z","iopub.status.idle":"2025-10-22T04:37:52.432394Z","shell.execute_reply.started":"2025-10-22T04:37:52.432218Z","shell.execute_reply":"2025-10-22T04:37:52.432238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quantised model","metadata":{"execution":{"iopub.status.busy":"2025-10-22T04:37:52.434348Z","iopub.status.idle":"2025-10-22T04:37:52.434826Z","shell.execute_reply.started":"2025-10-22T04:37:52.434614Z","shell.execute_reply":"2025-10-22T04:37:52.434635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model from checkpoint /kaggle/input/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\nmodel = tf.keras.models.load_model(\"/kaggle/working/asr-midtrained/tensorflow2/default/1/asr_model_final_ep15.keras\", custom_objects={\"ctc_loss\": ctc_loss})\n\n# (Optionally) Lower LR manually before continuing\ntf.keras.backend.set_value(model.optimizer.learning_rate, 1e-4)\n\n# Recreate the same callbacks\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=1,\n    min_lr=1e-6,\n    verbose=1\n)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"asr_model_best.keras\",\n    save_best_only=True,\n    monitor=\"val_loss\",\n    verbose=1\n)\n\n# Resume training from epoch 15 â†’ 25\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=25,\n    initial_epoch=15,\n    callbacks=[model_checkpoint, reduce_lr]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:37:52.438935Z","iopub.status.idle":"2025-10-22T04:37:52.440091Z","shell.execute_reply.started":"2025-10-22T04:37:52.439476Z","shell.execute_reply":"2025-10-22T04:37:52.439773Z"}},"outputs":[],"execution_count":null}]}